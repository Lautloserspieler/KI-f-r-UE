"""Verwaltung der LLM-Kommunikation (lokal oder via HTTP)."""

from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Sequence

import requests

from auto_pcg.core.asset_analyzer import AssetAnalyzer
from auto_pcg.models.schemas import AssetData, Classification, PCGFilterSpec, PCGLayer, PCGPlan

from .local_llm import LocalGGUFClient, LocalLLMError
from .prompt_engine import PromptEngine

LOGGER = logging.getLogger(__name__)


class LLMManager:
    """Kapselt sowohl lokale GGUF-Aufrufe als auch Ollama-kompatibles HTTP."""

    CLASSIFICATION_BATCH_SIZE = 10
    PCG_CONTEXT_LIMIT = 30

    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "llama3",
        timeout: float = 10.0,
        prompt_engine: Optional[PromptEngine] = None,
        local_model_path: Optional[Path] = None,
    ) -> None:
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = timeout
        self.session = requests.Session()
        self.prompt_engine = prompt_engine or PromptEngine()
        self._analyzer = AssetAnalyzer()
        self._local_client: Optional[LocalGGUFClient] = None
        if local_model_path:
            try:
                self._local_client = LocalGGUFClient(local_model_path)
                LOGGER.info("Verwende lokales GGUF-Modell: %s", local_model_path)
            except LocalLLMError as exc:
                LOGGER.warning("Lokales GGUF-Modell konnte nicht geladen werden: %s", exc)

    def setup_ollama_connection(self) -> bool:
        """Validiert, ob der Ollama-Endpunkt erreichbar ist."""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=self.timeout)
            response.raise_for_status()
            return True
        except requests.RequestException as exc:  # pragma: no cover - nur Netzwerkausnahme
            LOGGER.warning("Ollama nicht erreichbar: %s", exc)
            return False

    def send_classification_request(self, assets: Sequence[AssetData]) -> List[Classification]:
        """Sendet eine Klassifikationsanfrage oder nutzt Fallback-Heuristiken."""
        results: List[Classification] = []
        for batch_index, batch in enumerate(_chunked(assets, self.CLASSIFICATION_BATCH_SIZE)):
            prompt = self.prompt_engine.build_asset_classification_prompt(batch)
            parsed = self._parse_classifications(self._run_prompt(prompt), batch)
            if parsed:
                results.extend(parsed)
            else:
                LOGGER.info("Nutze lokale Fallback-Klassifikation (Batch %s).", batch_index)
                results.extend(self._analyzer.classify_asset_semantics(asset) for asset in batch)
        return results

    def send_pcg_generation_request(self, user_prompt: str, assets: Sequence[AssetData]) -> PCGPlan:
        """Generiert einen PCG-Plan über das LLM oder liefert einen simplen Fallback."""
        context_assets = assets[: self.PCG_CONTEXT_LIMIT]
        if len(assets) > self.PCG_CONTEXT_LIMIT:
            LOGGER.info(
                "Beschränke PCG-Kontext von %s auf %s Assets, um den Prompt klein zu halten.",
                len(assets),
                self.PCG_CONTEXT_LIMIT,
            )
        prompt = self.prompt_engine.build_pcg_generation_prompt(user_prompt, context_assets)
        plan = self._parse_pcg_plan(self._run_prompt(prompt))
        if plan:
            return plan
        LOGGER.info("Nutze lokalen PCG-Fallback.")
        return self._fallback_pcg_plan(user_prompt, assets)

    def validate_llm_response(self, response: Dict[str, object]) -> bool:
        """Stellt sicher, dass Kernfelder vorhanden sind."""
        if "pcg_plan" in response:
            plan = response["pcg_plan"]
            return isinstance(plan, dict) and "layers" in plan
        if "classifications" in response:
            value = response["classifications"]
            return isinstance(value, list)
        return False

    # Interne Hilfen ---------------------------------------------------------------------------

    def _run_prompt(self, prompt: str) -> Optional[Dict[str, object]]:
        """Routet Prompts an lokales GGUF oder an den HTTP-Endpunkt."""
        if self._local_client:
            try:
                return self._local_client.generate_json(prompt)
            except LocalLLMError as exc:
                LOGGER.error("Lokales LLM lieferte einen Fehler: %s", exc)
                return None
        return self._post_prompt_http(prompt)

    def _post_prompt_http(self, prompt: str) -> Optional[Dict[str, object]]:
        """Sendet einen Prompt an Ollama und dekodiert JSON."""
        try:
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                },
                timeout=self.timeout,
            )
            response.raise_for_status()
            payload = response.json()
            text = payload.get("response", "").strip()
            return json.loads(text)
        except (requests.RequestException, json.JSONDecodeError, KeyError) as exc:
            LOGGER.warning("LLM-Kommunikation fehlgeschlagen: %s", exc)
            return None

    def _fallback_pcg_plan(self, user_prompt: str, assets: Sequence[AssetData]) -> PCGPlan:
        """Generiert einen simplen PCG-Plan ohne LLM."""
        chosen_assets = list(assets)[:3]
        layers = [
            PCGLayer(
                layer_type="SURFACE",
                purpose="Grundflaeche fuer " + user_prompt,
                assets=[asset.asset_path for asset in chosen_assets[:1]],
                parameters={"tiling": 1.0, "roughness": 0.4},
            ),
            PCGLayer(
                layer_type="SCATTER",
                purpose="Streuung thematischer Assets",
                assets=[asset.asset_path for asset in chosen_assets],
                parameters={"density": 0.65, "scale_variation": [0.8, 1.3]},
                filters=[PCGFilterSpec(type="BySlope", params={"min": 0.1, "max": 0.7})],
            ),
        ]
        return PCGPlan(
            description=f"Fallback-Plan fuer '{user_prompt}'",
            target_biome="generic",
            layers=layers,
        )

    def _parse_classifications(
        self,
        payload: Optional[Dict[str, object]],
        assets: Sequence[AssetData],
    ) -> List[Classification]:
        """Validiert LLM-Output und wandelt ihn in Classification-Objekte um."""
        if not payload or "classifications" not in payload:
            return []
        entries = payload.get("classifications", [])
        if not isinstance(entries, list):
            LOGGER.warning("LLM-Antwort enthaelt kein Klassen-Array.")
            return []
        result: List[Classification] = []
        for index, item in enumerate(entries):
            try:
                if not isinstance(item, dict):
                    raise TypeError("Eintrag ist kein Objekt.")
                fallback_path = assets[index].asset_path if index < len(assets) else assets[0].asset_path
                asset_path = Path(item.get("asset_path") or fallback_path)
                primary_category = item.get("primary_category") or item.get("class") or "PROP"
                sub_category = item.get("sub_category") or item.get("subclass") or "Generic"
                tags = item.get("tags") or item.get("classifications") or []
                style = item.get("style") or item.get("visual_style") or "realistic"
                biomes = item.get("biomes") or item.get("biome") or []
                if isinstance(biomes, str):
                    biomes = [biomes]
                if not isinstance(tags, list):
                    tags = [str(tags)]
                classification = Classification(
                    asset_path=asset_path,
                    primary_category=str(primary_category),
                    sub_category=str(sub_category),
                    tags=[str(tag) for tag in tags],
                    style=str(style),
                    biomes=[str(biome) for biome in biomes],
                )
                result.append(classification)
            except (KeyError, IndexError, TypeError, ValueError) as exc:
                LOGGER.warning("Ungültige Klassifikation im LLM-Output (ignoriert): %s", exc)
        return result

    def _parse_pcg_plan(self, payload: Optional[Dict[str, object]]) -> Optional[PCGPlan]:
        """Validiert den LLM-Output für PCG-Pläne."""
        if not payload or "pcg_plan" not in payload:
            return None
        plan_data = payload["pcg_plan"]
        if not isinstance(plan_data, dict):
            LOGGER.warning("PCG-Plan ist kein Objekt.")
            return None
        raw_layers = plan_data.get("layers", [])
        if not isinstance(raw_layers, list) or not raw_layers:
            LOGGER.warning("PCG-Plan enthaelt keine Layer.")
            return None
        try:
            layers = [
                PCGLayer(
                    layer_type=layer["type"],
                    purpose=layer["purpose"],
                    assets=[Path(path) for path in layer.get("assets", [])],
                    parameters=layer.get("parameters", {}),
                    filters=[
                        PCGFilterSpec(filter_spec["type"], {k: v for k, v in filter_spec.items() if k != "type"})
                        for filter_spec in layer.get("filters", [])
                    ],
                )
                for layer in raw_layers
                if isinstance(layer, dict)
            ]
            if not layers:
                LOGGER.warning("Keine gueltigen Layer im PCG-Plan gefunden.")
                return None
            return PCGPlan(
                description=plan_data.get("description", "LLM-Plan"),
                target_biome=plan_data.get("target_biome", "unknown"),
                layers=layers,
            )
        except (KeyError, TypeError, ValueError) as exc:
            LOGGER.warning("PCG-Plan konnte nicht geparst werden: %s", exc)
            return None


def _chunked(sequence: Sequence[AssetData], size: int):
    """Teilt eine Sequenz in handliche Teilmengen auf."""
    if size <= 0:
        yield sequence
        return
    for i in range(0, len(sequence), size):
        yield sequence[i : i + size]


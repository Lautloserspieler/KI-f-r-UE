### src\auto_pcg\__init__.py
"""Auto-PCG KI-Assistent-Paket."""
from .services.pcg_service import AutoPCGService

__all__ = ["AutoPCGService"]

### src\auto_pcg\__main__.py
"""Ermöglicht `python -m auto_pcg`."""

from .cli import main

if __name__ == "__main__":
    main()

### src\auto_pcg\ai\__init__.py
"""Auto-PCG Modul."""

### src\auto_pcg\ai\llm_manager.py
"""Verwaltung der LLM-Kommunikation (lokal oder via HTTP)."""

from __future__ import annotations

import json
import logging
import math
import time
from pathlib import Path
from typing import Dict, List, Optional, Sequence

import requests

from auto_pcg.core.asset_analyzer import AssetAnalyzer
from auto_pcg.models.schemas import AssetData, Classification, PCGFilterSpec, PCGLayer, PCGPlan
from auto_pcg.models.terrain import HeightmapAnalysisResult, LandscapeLayerPlan, MaterialBlueprint

from .local_llm import LocalGGUFClient, LocalLLMError, _auto_close_json, _extract_json_block
from .prompt_engine import PromptEngine

LOGGER = logging.getLogger(__name__)


class LLMManager:
    """Kapselt sowohl lokale GGUF-Aufrufe als auch Ollama-kompatibles HTTP."""

    CLASSIFICATION_BATCH_SIZE = 10
    PCG_CONTEXT_LIMIT = 30

    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "llama3",
        timeout: float = 10.0,
        prompt_engine: Optional[PromptEngine] = None,
        local_model_path: Optional[Path] = None,
        classification_batch_size: Optional[int] = None,
    ) -> None:
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = timeout
        self.session = requests.Session()
        self.prompt_engine = prompt_engine or PromptEngine()
        self._analyzer = AssetAnalyzer()
        self._local_client: Optional[LocalGGUFClient] = None
        self._classification_batch_size = (
            max(1, classification_batch_size) if classification_batch_size else self.CLASSIFICATION_BATCH_SIZE
        )
        if local_model_path:
            try:
                self._local_client = LocalGGUFClient(local_model_path)
                LOGGER.info("Verwende lokales GGUF-Modell: %s", local_model_path)
            except LocalLLMError as exc:
                LOGGER.warning("Lokales GGUF-Modell konnte nicht geladen werden: %s", exc)

    def setup_ollama_connection(self) -> bool:
        """Validiert, ob der Ollama-Endpunkt erreichbar ist."""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=self.timeout)
            response.raise_for_status()
            return True
        except requests.RequestException as exc:  # pragma: no cover - nur Netzwerkausnahme
            LOGGER.warning("Ollama nicht erreichbar: %s", exc)
            return False

    def send_classification_request(self, assets: Sequence[AssetData]) -> List[Classification]:
        """Sendet eine Klassifikationsanfrage oder nutzt Fallback-Heuristiken."""
        results: List[Classification] = []
        total_batches = math.ceil(len(assets) / self._classification_batch_size) or 1
        for batch_index, batch in enumerate(_chunked(assets, self._classification_batch_size)):
            LOGGER.info(
                "LLM-Klassifikation Batch %s/%s (%s Assets)...",
                batch_index + 1,
                total_batches,
                len(batch),
            )
            start = time.perf_counter()
            prompt = self.prompt_engine.build_asset_classification_prompt(batch)
            parsed = self._parse_classifications(self._run_prompt(prompt), batch)
            if parsed:
                duration = time.perf_counter() - start
                LOGGER.info(
                    "LLM-Klassifikation Batch %s abgeschlossen (%.1fs, %s Ergebnisse).",
                    batch_index + 1,
                    duration,
                    len(parsed),
                )
                results.extend(parsed)
            else:
                duration = time.perf_counter() - start
                LOGGER.info("Nutze lokale Fallback-Klassifikation (Batch %s).", batch_index)
                results.extend(self._analyzer.classify_asset_semantics(asset) for asset in batch)
        return results

    def send_pcg_generation_request(self, user_prompt: str, assets: Sequence[AssetData]) -> PCGPlan:
        """Generiert einen PCG-Plan über das LLM oder liefert einen simplen Fallback."""
        context_assets = assets[: self.PCG_CONTEXT_LIMIT]
        if len(assets) > self.PCG_CONTEXT_LIMIT:
            LOGGER.info(
                "Beschränke PCG-Kontext von %s auf %s Assets, um den Prompt klein zu halten.",
                len(assets),
                self.PCG_CONTEXT_LIMIT,
            )
        prompt = self.prompt_engine.build_pcg_generation_prompt(user_prompt, context_assets)
        LOGGER.info("LLM-PCG-Anfrage gestartet (Prompt: %s , Assets: %s).", user_prompt, len(context_assets))
        start = time.perf_counter()
        plan = self._parse_pcg_plan(self._run_prompt(prompt), context_assets)
        if plan:
            LOGGER.info("LLM-PCG-Antwort erhalten (%.1fs).", time.perf_counter() - start)
            return plan
        LOGGER.info("Nutze lokalen PCG-Fallback.")
        return self._fallback_pcg_plan(user_prompt, assets)

    # Terrain-Workflows ----------------------------------------------------------------------

    def plan_heightmap_strategy(
        self,
        analysis: HeightmapAnalysisResult,
    ) -> Optional[Dict[str, object]]:
        """Fragt das LLM nach Optimierungen für Heightmap/Biome."""
        prompt = self.prompt_engine.build_heightmap_strategy_prompt(analysis)
        payload = self._run_prompt(prompt)
        if not payload:
            return None
        strategy = payload.get("heightmap_strategy") if isinstance(payload, dict) else None
        if not isinstance(strategy, dict):
            return None
        return strategy

    def plan_material_blueprint(
        self,
        analysis: HeightmapAnalysisResult,
        blueprint: MaterialBlueprint,
    ) -> Optional[Dict[str, object]]:
        """Lässt das LLM Material-Layer Vorschläge liefern."""
        prompt = self.prompt_engine.build_material_blueprint_prompt(analysis, blueprint)
        payload = self._run_prompt(prompt)
        if not payload:
            return None
        result = payload.get("material_blueprint") if isinstance(payload, dict) else None
        if not isinstance(result, dict):
            return None
        return result

    def plan_layer_paint(self, plan: LandscapeLayerPlan) -> Optional[Dict[str, object]]:
        """Fragt das LLM nach Layer-Mask-Optimierungen."""
        prompt = self.prompt_engine.build_layer_paint_prompt(plan)
        payload = self._run_prompt(prompt)
        if not payload:
            return None
        result = payload.get("layer_plan") if isinstance(payload, dict) else None
        if not isinstance(result, dict):
            return None
        return result

    def validate_llm_response(self, response: Dict[str, object]) -> bool:
        """Stellt sicher, dass Kernfelder vorhanden sind."""
        if "pcg_plan" in response:
            plan = response["pcg_plan"]
            return isinstance(plan, dict) and "layers" in plan
        if "classifications" in response:
            value = response["classifications"]
            return isinstance(value, list)
        return False

    # Interne Hilfen ---------------------------------------------------------------------------

    def _run_prompt(self, prompt: str) -> Optional[Dict[str, object]]:
        """Routet Prompts an lokales GGUF oder an den HTTP-Endpunkt."""
        if self._local_client:
            try:
                return self._local_client.generate_json(prompt)
            except LocalLLMError as exc:
                LOGGER.error("Lokales LLM lieferte einen Fehler: %s", exc)
                return None
        return self._post_prompt_http(prompt)

    def _post_prompt_http(self, prompt: str) -> Optional[Dict[str, object]]:
        """Sendet einen Prompt an Ollama und dekodiert JSON."""
        last_exc: Optional[Exception] = None
        for mode in ("chat", "generate"):
            try:
                if mode == "chat":
                    response = self.session.post(
                        f"{self.base_url}/api/chat",
                        json={
                            "model": self.model,
                            "messages": [
                                {"role": "system", "content": "Antwort exakt mit gültigem JSON."},
                                {"role": "user", "content": prompt},
                            ],
                            "stream": False,
                            "response_format": {"type": "json_object"},
                        },
                        timeout=self.timeout,
                    )
                    response.raise_for_status()
                    payload = response.json()
                    text = payload.get("message", {}).get("content", "").strip()
                else:
                    response = self.session.post(
                        f"{self.base_url}/api/generate",
                        json={
                            "model": self.model,
                            "prompt": prompt,
                            "stream": False,
                            "format": "json",
                        },
                        timeout=self.timeout,
                    )
                    response.raise_for_status()
                    payload = response.json()
                    text = payload.get("response", "").strip()
                if not text:
                    LOGGER.warning("Ollama-%s-Antwort enthielt keinen Text.", mode)
                    continue
                decoded = self._decode_llm_json(text)
                if decoded is None:
                    raise json.JSONDecodeError("invalid json", text, 0)
                return decoded
            except (requests.RequestException, json.JSONDecodeError, KeyError) as exc:
                last_exc = exc
                LOGGER.warning("Ollama-%s-Request fehlgeschlagen: %s", mode, exc)
        if last_exc:
            LOGGER.warning("LLM-Kommunikation fehlgeschlagen: %s", last_exc)
        return None

    def _decode_llm_json(self, text: str) -> Optional[Dict[str, object]]:
        trimmed = text.strip()
        if not trimmed:
            return None
        try:
            return json.loads(trimmed)
        except json.JSONDecodeError:
            try:
                extracted = _extract_json_block(trimmed)
                repaired = _auto_close_json(extracted)
                return json.loads(repaired)
            except json.JSONDecodeError:
                return None

    def _infer_layer_type(self, payload: Dict[str, object]) -> Optional[str]:
        """Versucht, fehlende Layer-Typen aus Feldern abzuleiten."""
        if not isinstance(payload, dict):
            return None
        layer_id = str(payload.get("layer_id", "")).lower()
        assets = payload.get("assets") or []
        has_assets = isinstance(assets, list) and bool(assets)
        if "surface" in layer_id or "ground" in layer_id:
            return "SURFACE"
        if "scatter" in layer_id or "foliage" in layer_id:
            return "SCATTER"
        if has_assets and ("count_min" in payload or "count_max" in payload):
            return "SCATTER"
        probability = payload.get("probability")
        if has_assets and probability is not None:
            return "SCATTER"
        if payload.get("template"):
            return "SCATTER"
        return None

    def _normalize_layer_type(self, layer_type: str, payload: Optional[Dict[str, object]] = None) -> str:
        value = str(layer_type).lower()
        mapping = {
            "surface": "SURFACE",
            "ground": "SURFACE",
            "terrain": "SURFACE",
            "worldmaplayer": "SURFACE",
            "worldmaplayermerged": "SURFACE",
            "scatter": "SCATTER",
            "tree": "SCATTER",
            "sapling": "SCATTER",
            "seedling": "SCATTER",
            "vegetation": "SCATTER",
            "instance": "SCATTER",
            "hlodlayerinstanced": "SCATTER",
        }
        if value in mapping:
            return mapping[value]
        if payload:
            assets = payload.get("assets") or []
            if isinstance(assets, list) and assets:
                return "SCATTER"
        return layer_type if isinstance(layer_type, str) else str(layer_type).upper()

    def _resolve_assets(self, raw_assets, context_assets: Sequence[AssetData], payload: Optional[Dict[str, object]] = None) -> List[Path]:
        """Mappt Asset-Namen aus dem Plan auf echte Pfade."""
        candidates = list(raw_assets or [])
        if payload and payload.get("template"):
            candidates.append(payload["template"])
        name_to_asset = {
            asset.asset_path.stem.lower(): asset.asset_path
            for asset in context_assets
        }
        resolved: List[Path] = []
        for entry in candidates:
            if isinstance(entry, str):
                key = entry.split("/")[-1].split("\\")[-1].split(".")[0].lower()
                match = name_to_asset.get(key)
                if match:
                    resolved.append(match)
                else:
                    resolved.append(Path(entry))
        return resolved

    def _fallback_pcg_plan(self, user_prompt: str, assets: Sequence[AssetData]) -> PCGPlan:
        """Generiert einen simplen PCG-Plan ohne LLM."""
        chosen_assets = list(assets)[:3]
        layers = [
            PCGLayer(
                layer_type="SURFACE",
                purpose="Grundflaeche fuer " + user_prompt,
                assets=[asset.asset_path for asset in chosen_assets[:1]],
                parameters={"tiling": 1.0, "roughness": 0.4},
            ),
            PCGLayer(
                layer_type="SCATTER",
                purpose="Streuung thematischer Assets",
                assets=[asset.asset_path for asset in chosen_assets],
                parameters={"density": 0.65, "scale_variation": [0.8, 1.3]},
                filters=[PCGFilterSpec(type="BySlope", params={"min": 0.1, "max": 0.7})],
            ),
        ]
        return PCGPlan(
            description=f"Fallback-Plan fuer '{user_prompt}'",
            target_biome="generic",
            layers=layers,
        )

    def _parse_classifications(
        self,
        payload: Optional[Dict[str, object]],
        assets: Sequence[AssetData],
    ) -> List[Classification]:
        """Validiert LLM-Output und wandelt ihn in Classification-Objekte um."""
        if not payload or "classifications" not in payload:
            return []
        entries = payload.get("classifications", [])
        if not isinstance(entries, list):
            LOGGER.warning("LLM-Antwort enthaelt kein Klassen-Array.")
            return []
        result: List[Classification] = []
        for index, item in enumerate(entries):
            try:
                if not isinstance(item, dict):
                    raise TypeError("Eintrag ist kein Objekt.")
                fallback_path = assets[index].asset_path if index < len(assets) else assets[0].asset_path
                asset_path = Path(item.get("asset_path") or fallback_path)
                primary_category = item.get("primary_category") or item.get("class") or "PROP"
                sub_category = item.get("sub_category") or item.get("subclass") or "Generic"
                tags = item.get("tags") or item.get("classifications") or []
                style = item.get("style") or item.get("visual_style") or "realistic"
                biomes = item.get("biomes") or item.get("biome") or []
                technical = item.get("technical") or {}
                if isinstance(biomes, str):
                    biomes = [biomes]
                if not isinstance(tags, list):
                    tags = [str(tags)]
                if not isinstance(technical, dict):
                    technical = {}
                classification = Classification(
                    asset_path=asset_path,
                    primary_category=str(primary_category),
                    sub_category=str(sub_category),
                    tags=[str(tag) for tag in tags],
                    style=str(style),
                    biomes=[str(biome) for biome in biomes],
                    technical=technical,
                )
                result.append(classification)
            except (KeyError, IndexError, TypeError, ValueError) as exc:
                LOGGER.warning("Ungültige Klassifikation im LLM-Output (ignoriert): %s", exc)
        return result

    def _parse_pcg_plan(self, payload: Optional[Dict[str, object]], context_assets: Sequence[AssetData]) -> Optional[PCGPlan]:
        """Validiert den LLM-Output für PCG-Pläne."""
        if not payload or "pcg_plan" not in payload:
            return None
        plan_data = payload["pcg_plan"]
        if not isinstance(plan_data, dict):
            LOGGER.warning("PCG-Plan ist kein Objekt.")
            return None
        raw_layers = plan_data.get("layers", [])
        if not isinstance(raw_layers, list) or not raw_layers:
            LOGGER.warning("PCG-Plan enthaelt keine Layer.")
            return None
        try:
            layers = []
            for raw in raw_layers:
                if not isinstance(raw, dict):
                    continue
                layer_type = (
                    raw.get("type")
                    or raw.get("layer_type")
                    or self._infer_layer_type(raw)
                )
                if not layer_type:
                    LOGGER.warning("PCG-Layer ohne 'type' verworfen: %s", raw)
                    continue
                layer_type = self._normalize_layer_type(layer_type, raw)
                purpose = raw.get("purpose") or raw.get("description") or layer_type
                assets = self._resolve_assets(raw.get("assets", []), context_assets, raw)
                parameters = dict(raw.get("parameters", {}))
                for key in ("count_min", "count_max", "probability", "density"):
                    if key in raw and key not in parameters:
                        parameters[key] = raw[key]
                filters = [
                    PCGFilterSpec(
                        filter_spec.get("type", "Unknown"),
                        {k: v for k, v in filter_spec.items() if k != "type"},
                    )
                    for filter_spec in raw.get("filters", [])
                    if isinstance(filter_spec, dict)
                ]
                layers.append(
                    PCGLayer(
                        layer_type=str(layer_type),
                        purpose=str(purpose),
                        assets=assets,
                        parameters=parameters,
                        filters=filters,
                    )
                )
            if not layers:
                LOGGER.warning("Keine gueltigen Layer im PCG-Plan gefunden.")
                return None
            return PCGPlan(
                description=plan_data.get("description", "LLM-Plan"),
                target_biome=plan_data.get("target_biome", "unknown"),
                layers=layers,
            )
        except (KeyError, TypeError, ValueError) as exc:
            LOGGER.warning("PCG-Plan konnte nicht geparst werden: %s", exc)
            return None


def _chunked(sequence: Sequence[AssetData], size: int):
    """Teilt eine Sequenz in handliche Teilmengen auf."""
    if size <= 0:
        yield sequence
        return
    for i in range(0, len(sequence), size):
        yield sequence[i : i + size]

### src\auto_pcg\ai\local_llm.py
"""Einfacher Wrapper um llama.cpp für lokale GGUF-Modelle."""

from __future__ import annotations

import json
import logging
import os
import re
from pathlib import Path
from typing import Dict, Optional

try:
    from llama_cpp import Llama  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    Llama = None

LOGGER = logging.getLogger(__name__)


class LocalLLMError(RuntimeError):
    """Signalisiert Fehler in der lokalen LLM-Ausführung."""


class LocalGGUFClient:
    """Hilfsklasse, die direkt ein GGUF-Modell über llama.cpp lädt."""

    def __init__(
        self,
        model_path: Path,
        context_tokens: int = 4096,
        max_tokens: int = 768,
        temperature: float = 0.15,
        chat_format: str = "llama-3",
        n_gpu_layers: Optional[int] = None,
    ) -> None:
        if Llama is None:
            raise LocalLLMError(
                "llama-cpp-python ist nicht installiert. Bitte `pip install llama-cpp-python` ausführen."
            )
        if not model_path.exists():
            raise LocalLLMError(f"GGUF-Modell nicht gefunden: {model_path}")
        self.model_path = model_path
        self.temperature = temperature
        self.max_tokens = max_tokens
        gpu_layers = self._resolve_gpu_layers(n_gpu_layers)
        self._llama = self._init_llama(
            model_path=model_path,
            context_tokens=context_tokens,
            chat_format=chat_format,
            gpu_layers=gpu_layers,
        )

    def generate_json(self, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, object]:
        """Führt eine Chat Completion aus und gibt JSON zurück."""
        system_prompt = system_prompt or (
            "Du bist ein strikter JSON-Generator für den Auto-PCG KI-Assistenten. "
            "Antworte ausschließlich mit gültigem JSON."
        )
        response = self._llama.create_chat_completion(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )
        content = response["choices"][0]["message"]["content"].strip()
        content = _extract_json_block(content)
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            repaired = _auto_close_json(content)
            try:
                return json.loads(repaired)
            except json.JSONDecodeError as exc:
                LOGGER.error(
                    "LLM-Antwort konnte nicht als JSON geparst werden: %s\nAntwort: %s",
                    exc,
                    content,
                )
                raise LocalLLMError("Ungültige JSON-Antwort durch das GGUF-Modell") from exc


    def _init_llama(
        self,
        *,
        model_path: Path,
        context_tokens: int,
        chat_format: str,
        gpu_layers: int,
    ) -> "Llama":
        """Initialisiert das Llama-Objekt und fällt bei Fehlern auf CPU zurück."""
        use_gpu = gpu_layers != 0
        if use_gpu:
            LOGGER.info(
                "Initialisiere GGUF-Client mit %s GPU-Layern.",
                "allen" if gpu_layers < 0 else gpu_layers,
            )
        try:
            return Llama(
                model_path=str(model_path),
                n_ctx=context_tokens,
                chat_format=chat_format,
                n_threads=os.cpu_count() or 4,
                n_gpu_layers=gpu_layers,
            )
        except (OSError, RuntimeError) as exc:
            if use_gpu:
                LOGGER.warning(
                    "Konnte GGUF-Modell nicht mit GPU-Layern laden (%s). Fallback auf CPU.",
                    exc,
                )
                try:
                    return Llama(
                        model_path=str(model_path),
                        n_ctx=context_tokens,
                        chat_format=chat_format,
                        n_threads=os.cpu_count() or 4,
                        n_gpu_layers=0,
                    )
                except (OSError, RuntimeError) as cpu_exc:
                    raise LocalLLMError(f"LLM-Initialisierung fehlgeschlagen (CPU-Fallback): {cpu_exc}") from cpu_exc
            raise LocalLLMError(f"LLM-Initialisierung fehlgeschlagen: {exc}") from exc

    @staticmethod
    def _resolve_gpu_layers(override: Optional[int]) -> int:
        """Liest die Anzahl der GPU-Layer aus Parameter oder Umgebungsvariable."""
        if override is not None:
            return override
        env_value = os.getenv("AUTO_PCG_GPU_LAYERS")
        if env_value is None or not env_value.strip():
            return -1
        try:
            return int(env_value)
        except ValueError:
            LOGGER.warning(
                "AUTO_PCG_GPU_LAYERS=%s konnte nicht interpretiert werden. Verwende -1.",
                env_value,
            )
            return -1

def _extract_json_block(text: str) -> str:
    """Entfernt Code-Fences und versucht, nur den JSON-Teil zu extrahieren."""
    stripped = text.strip()
    if stripped.startswith("```"):
        stripped = re.sub(r"^```(?:json)?", "", stripped, flags=re.IGNORECASE).strip()
        stripped = re.sub(r"```$", "", stripped).strip()
    if stripped.startswith("{") or stripped.startswith("["):
        return stripped
    match = re.search(r"(\{.*\})", stripped, re.DOTALL)
    if match:
        return match.group(1)
    match = re.search(r"(\[.*\])", stripped, re.DOTALL)
    if match:
        return match.group(1)
    return stripped



def _append_missing_closers(text: str) -> str:
    """Hängt fehlende schließende Klammern basierend auf einer Stack-Analyse an."""
    stack: list[str] = []
    in_string = False
    escape = False
    for char in text:
        if in_string:
            if escape:
                escape = False
            elif char == "\\":
                escape = True
            elif char == '"':
                in_string = False
            continue
        if char == '"':
            in_string = True
            continue
        if char in "{[":
            stack.append(char)
        elif char in "}]":
            if stack and ((char == "}" and stack[-1] == "{") or (char == "]" and stack[-1] == "[")):
                stack.pop()
            else:
                break
    closing = "".join("}" if ch == "{" else "]" for ch in reversed(stack))
    return text + closing


def _auto_close_json(text: str) -> str:
    """Versucht, fehlende Klammern am Ende zu ergänzen."""
    candidate = _append_missing_closers(text)
    try:
        json.loads(candidate)
        return candidate
    except json.JSONDecodeError:
        return _repair_json_lines(candidate)


def _repair_json_lines(text: str) -> str:
    """Versucht, abgeschnittene JSON-Dokumente aggressiver zu reparieren."""
    parsed = _try_parse_with_trimming(text)
    if parsed is not None:
        return json.dumps(_prune_classifications(parsed), ensure_ascii=False)

    if '"asset_path"' in text:
        entries = _extract_objects_with_key(text, "asset_path")
        if entries:
            payload = {"classifications": entries}
            return json.dumps(_prune_classifications(payload), ensure_ascii=False)

    return text


def _try_parse_with_trimming(text: str) -> Optional[object]:
    """Schneidet sukzessive ungültige Enden ab und versucht nach jedem Schritt zu parsen."""
    candidate = text
    while candidate:
        try:
            return json.loads(candidate)
        except json.JSONDecodeError as exc:
            trim_point = _find_trim_point(candidate, exc.pos)
            if trim_point is None:
                break
            trimmed = candidate[:trim_point].rstrip()
            if not trimmed:
                break
            sanitized = _strip_dangling_openers(_strip_trailing_comma(trimmed)).rstrip()
            if not sanitized:
                break
            candidate = _append_missing_closers(sanitized)
    return None


def _find_trim_point(text: str, error_pos: int) -> Optional[int]:
    """Bestimmt die Position, bis zu der Text abgeschnitten werden soll."""
    if error_pos <= 0:
        return None
    newline_index = text.rfind("\n", 0, error_pos)
    if newline_index != -1:
        return newline_index
    return error_pos - 1 if error_pos > 0 else None


def _strip_trailing_comma(text: str) -> str:
    """Entfernt ein finales Komma inklusive Leerraum."""
    return re.sub(r",\s*$", "", text)


def _strip_dangling_openers(text: str) -> str:
    """Entfernt offene geschweifte oder eckige Klammern am Ende."""
    return re.sub(r"[\t ]*[\{\[]\s*$", "", text)


def _extract_objects_with_key(text: str, key: str) -> list[Dict[str, object]]:
    """Extrahiert vollständige JSON-Objekte, die den angegebenen Schlüssel enthalten."""
    results: list[Dict[str, object]] = []
    brace_stack: list[int] = []
    in_string = False
    escape = False
    for idx, char in enumerate(text):
        if in_string:
            if escape:
                escape = False
            elif char == "\\":
                escape = True
            elif char == '"':
                in_string = False
            continue
        if char == '"':
            in_string = True
            continue
        if char == "{":
            brace_stack.append(idx)
        elif char == "}" and brace_stack:
            start = brace_stack.pop()
            chunk = text[start : idx + 1]
            if f'"{key}"' not in chunk:
                continue
            try:
                obj = json.loads(chunk)
            except json.JSONDecodeError:
                continue
            if isinstance(obj, dict) and key in obj:
                results.append(obj)
    return results


def _prune_classifications(payload: object) -> object:
    """Entfernt Klassifikationsobjekte ohne asset_path, falls vorhanden."""
    if isinstance(payload, dict):
        entries = payload.get("classifications")
        if isinstance(entries, list):
            payload = payload.copy()
            payload["classifications"] = [
                entry for entry in entries if isinstance(entry, dict) and entry.get("asset_path")
            ]
    return payload

### src\auto_pcg\ai\prompt_engine.py
"""Prompt generation for classifications, PCG plans and terrain workflows."""

from __future__ import annotations

import json
import textwrap
from dataclasses import asdict
from typing import Iterable, Sequence

from auto_pcg.models.schemas import AssetData
from auto_pcg.models.terrain import (
    HeightmapAnalysisResult,
    LandscapeLayerPlan,
    MaterialBlueprint,
)


class PromptEngine:
    """Produces JSON-only prompts tailored to the Auto-PCG workflow."""

    # Asset & PCG Prompts --------------------------------------------------------------------

    def build_asset_classification_prompt(self, assets: Sequence[AssetData]) -> str:
        """Creates a JSON-centric prompt requesting semantic asset categories."""
        asset_snippets = [
            {
                "asset_path": str(asset.asset_path),
                "asset_type": asset.asset_type,
                "metadata": asdict(asset.metadata),
            }
            for asset in assets
        ]
        asset_json = json.dumps(asset_snippets, ensure_ascii=False, indent=2)
        instructions = textwrap.dedent(
            """
ROLLE: Du bist ein Asset-Klassifikator für Unreal Engine 5.4.
AUFGABE: Ordne jedes Asset mehreren Ebenen zu.

Hauptkategorien (Primary Categories):
- LANDSCAPE (Terrain, Wasser, Höhlen, Klippen, Pfade)
- VEGETATION (Bäume, Büsche, Gräser, Blumen, Pilze, Pflanzen)
- ARCHITECTURE (Gebäude, Ruinen, Brücken, Mauern, Böden, Dächer)
- PROP (Möbel, Container, Waffen, Werkzeuge, Dekoration, Licht)
- CHARACTER (Menschen, Tiere, Kreaturen, Monster)
- EFFECTS (Partikel, Licht, Sound)
- MATERIAL (Oberflächen/Materialdefinitionen)
- BLUEPRINT (Interaktive/technische Blueprints)

Unterkategorien: Wähle passende Begriffe aus den obigen Beispielen (z.B. Tree, Bush,
Terrain, Cliff, Building, Bridge, Furniture, Weapon, Particle, Ground, Interactive).

Tags: Liste aus beschreibenden Begriffen wie Größe (small/medium/large/huge),
Farbe (red/green/dark/bright), Zustand (new/old/broken), Jahreszeit, Umgebung
(indoor/outdoor/underground/underwater), Genre (magical, futuristic, medieval usw.).

Stil (Style): wähle aus {realistic, fantasy, sci-fi, medieval, modern, cartoon,
low-poly, stylized}.

Biomes: kombiniere die am besten passenden aus {forest, desert, mountain, grassland,
tundra, jungle, urban, aquatic, arctic, volcanic}.

Technische Eigenschaften (technical):
- polycount (Ganzzahl, z.B. Vertex-Anzahl)
- texture_resolution (Array [width, height], Ableitung aus Metadaten)
- collision ("simple", "complex" oder "none")
- lod (true/false)
- material_count (Ganzzahl)
Nutze die gelieferten Asset-Metadaten, um vernünftige Werte vorzuschlagen. Wenn
Informationen fehlen, triff eine begründete Schätzung.

ANTWORTFORMAT: Gib ausschließlich valides JSON ohne Markdown zurück:
{"classifications": [ ... ]}. Falls keine Klassifikation möglich ist, antworte mit
{"classifications": []}.
"""
        ).strip()
        return f"{instructions}\nASSETS:\n{asset_json}"

    def build_pcg_generation_prompt(self, user_input: str, context: Sequence[AssetData]) -> str:
        """Creates the LLM prompt for PCG layer planning."""
        asset_list = ", ".join(asset.asset_path.stem for asset in context)
        return (
            "ROLLE: Du bist ein PCG-Architekt für Unreal Engine 5.4.\n"
            f"AUFGABE: Erstelle einen PCG-Plan für den Befehl: {user_input}.\n"
            f"VERFÜGBARE ASSETS: {asset_list}.\n"
            "ANTWORTFORMAT: Gib ausschließlich reines JSON mit dem Objekt 'pcg_plan' zurück. "
            "Kein erläuternder Text, keine Codeblöcke. Falls nicht möglich, antworte mit "
            '{"pcg_plan": {"description": "", "target_biome": "unknown", "layers": []}}.'
        )

    def build_asset_selection_prompt(self, biome_type: str, available_assets: Iterable[AssetData]) -> str:
        """Creates a helper prompt for biome-specific asset recommendations."""
        candidates = [
            {
                "path": str(asset.asset_path),
                "tags": asset.semantic_tags,
                "biomes": asset.semantic_tags,
            }
            for asset in available_assets
        ]
        candidates_json = json.dumps(candidates, ensure_ascii=False, indent=2)
        return (
            "Bitte schlage passende Assets für das folgende Biom vor.\n"
            f"BIOM: {biome_type}\n"
            f"KANDIDATEN:\n{candidates_json}\n"
            "ANTWORT: Nur JSON ohne zusätzlichen Text."
        )

    # Terrain & Material Prompts --------------------------------------------------------------

    def build_heightmap_strategy_prompt(self, analysis: HeightmapAnalysisResult) -> str:
        """Describes the heightmap state and asks for strategic improvements."""
        payload = analysis.to_dict()
        return textwrap.dedent(
            f"""
ROLLE: Du bist ein Landscape Technical Director für Unreal Engine 5.4.
AUFGABE: Analysiere die gelieferten Heightmap-Daten und liefere Empfehlungen
für Landscape-Settings, Biome-Zuordnung und Skalierung.

VORGABEN:
- Antworte ausschließlich mit JSON im Format {{"heightmap_strategy": {{...}}}}
- Füge Felder wie "recommended_biomes", "landscape_settings" und "notes" hinzu.
- Ergänze Werte nur wenn du dir sicher bist, ansonsten lasse sie weg.

DATEN:
{json.dumps(payload, ensure_ascii=False, indent=2)}
"""
        ).strip()

    def build_material_blueprint_prompt(
        self,
        analysis: HeightmapAnalysisResult,
        blueprint: MaterialBlueprint,
    ) -> str:
        """Requests Material-Graph improvements for the detected biomes."""
        payload = {
            "analysis": analysis.to_dict(),
            "current_blueprint": blueprint.to_dict(),
        }
        return textwrap.dedent(
            f"""
ROLLE: Du bist ein UE5-Materialguru.
AUFGABE: Verbessere den Material-Blueprint für die erkannten Biome.
ANTWORTFORMAT: {{"material_blueprint": {{"layers": [...], "global_parameters": {{...}} }} }}
EINTRÄGE SOLLEN MIT LANDSCAPE LAYER BLEND KOMPATIBEL SEIN.

DATEN:
{json.dumps(payload, ensure_ascii=False, indent=2)}
"""
        ).strip()

    def build_layer_paint_prompt(self, plan: LandscapeLayerPlan) -> str:
        """Asks the LLM to optimise mask ordering and adaptive rules."""
        payload = plan.to_dict()
        return textwrap.dedent(
            f"""
ROLLE: Du bist eine Echtzeit-Painting-KI für UE Landscapes.
AUFGABE: Optimiere Layer-Masken und adaptive Regeln.
ANTWORTFORMAT: {{"layer_plan": {{"masks": [...], "adaptive_rules": {{...}} }} }}

PLAN:
{json.dumps(payload, ensure_ascii=False, indent=2)}
"""
        ).strip()

### src\auto_pcg\cli.py
"""Command line entry point for the Auto-PCG system."""

from __future__ import annotations

import argparse
import json
import logging
from pathlib import Path
from typing import Any, Dict

from auto_pcg.services.pcg_service import AutoPCGService

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Auto-PCG KI-Assistent")
    parser.add_argument("--project", type=Path, default=Path("."), help="Wurzelverzeichnis des Unreal-Projekts")
    parser.add_argument("--prompt", type=str, default="Erstelle einen dichten Wald", help="Natürlicher Sprachbefehl")
    parser.add_argument("--max-assets", type=int, default=None, help="Maximale Anzahl an Assets für den Scan")
    parser.add_argument("--batch-size", type=int, default=None, help="Anzahl Assets pro LLM-Klassifikationsbatch")
    parser.add_argument(
        "--heuristic-only",
        action="store_true",
        help="Überspringt das LLM und nutzt nur heuristische Klassifikation (am schnellsten)",
    )
    parser.add_argument(
        "--export-graph",
        type=Path,
        default=None,
        help="Optionales Zielverzeichnis, in das Graph-/Materialdaten exportiert werden",
    )
    parser.add_argument("--heightmap", type=Path, default=None, help="Pfad zu einer Heightmap-Datei")
    parser.add_argument(
        "--performance-profile",
        type=str,
        default="desktop",
        choices=["mobile", "desktop", "console", "high_end_pc"],
        help="Optimierungsziel für Landscape-/Material-Parameter",
    )
    parser.add_argument(
        "--target-style",
        type=str,
        default="realistic",
        help="Zielstil der Landschaft (z. B. realistic, stylized, tundra, desert)",
    )
    parser.add_argument("--ollama-url", type=str, default=None, help="Optionaler Ollama-Endpunkt")
    parser.add_argument("--ollama-model", type=str, default=None, help="Name des Ollama-Modells (z. B. llama3)")
    parser.add_argument(
        "--ollama-timeout",
        type=float,
        default=None,
        help="HTTP-Timeout für Ollama-Requests in Sekunden (Standard 10)",
    )
    parser.add_argument(
        "--no-local-model",
        action="store_true",
        help="Deaktiviert das lokale GGUF-Modell und nutzt ausschließlich Ollama",
    )
    parser.add_argument(
        "--no-layer-paint",
        action="store_true",
        help="Deaktiviert automatische Landscape-Layer-Planung",
    )
    parser.add_argument("--ue-editor", type=Path, default=None, help="Pfad zur UnrealEditor-Cmd.exe")
    parser.add_argument("--ue-map", type=str, default=None, help="Optionaler Map-Pfad (z. B. /Game/Maps/Example)")
    parser.add_argument(
        "--ue-asset-folder",
        type=str,
        default="/Game/AutoPCG",
        help="Content-Browser-Pfad für importierte Assets",
    )
    parser.add_argument(
        "--ue-no-spawn",
        action="store_true",
        help="Nur Graphen importieren, kein PCG-Volume im Level anlegen",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    service = AutoPCGService(
        args.project,
        max_assets=args.max_assets,
        prefer_heuristics=args.heuristic_only,
        classification_batch_size=args.batch_size,
        ollama_url=args.ollama_url,
        ollama_model=args.ollama_model,
        ollama_timeout=args.ollama_timeout,
        use_local_model=not args.no_local_model,
        export_directory=args.export_graph,
        heightmap=args.heightmap,
        performance_profile=args.performance_profile,
        target_style=args.target_style,
        auto_layer_paint=not args.no_layer_paint,
        ue_editor=args.ue_editor,
        ue_map=args.ue_map,
        ue_asset_folder=args.ue_asset_folder,
        ue_spawn=not args.ue_no_spawn,
    )

    logging.info("Starte vollautomatische KI-Pipeline...")
    result = service.run_full_pipeline(args.prompt)
    logging.info("Pipeline abgeschlossen.")
    payload = _serialize_result(result)
    print(json.dumps(payload, indent=2, ensure_ascii=False))


def _serialize_result(result: Dict[str, Any]) -> Dict[str, Any]:
    graph = result.get("graph")
    assets = result.get("assets") or []
    analysis = result.get("heightmap_analysis")
    blueprint = result.get("material_blueprint")
    layer_plan = result.get("layer_plan")
    return {
        "graph": _graph_to_dict(graph) if graph else None,
        "assets": [asset.to_dict() for asset in assets],
        "heightmap_analysis": analysis.to_dict() if analysis else None,
        "material_blueprint": blueprint.to_dict() if blueprint else None,
        "layer_plan": layer_plan.to_dict() if layer_plan else None,
    }


def _graph_to_dict(graph):
    return {
        "generated_at": graph.generated_at.isoformat(),
        "description": graph.description,
        "nodes": [_node_to_dict(node) for node in graph.root_nodes],
    }


def _node_to_dict(node):
    return {
        "name": node.name,
        "type": node.layer_type,
        "config": node.config,
        "children": [_node_to_dict(child) for child in node.children],
    }


if __name__ == "__main__":
    main()

### src\auto_pcg\core\__init__.py
"""Auto-PCG Modul."""

### src\auto_pcg\core\asset_analyzer.py
"""Semantische Analyse und Klassifikation von Assets."""

from __future__ import annotations

from typing import Dict, List

from auto_pcg.models.schemas import AssetData, Classification


class AssetAnalyzer:
    """Einfache Heuristiken zur Einordnung von Assets."""

    CATEGORY_BY_TYPE = {
        "StaticMesh": ("VEGETATION", "Foliage"),
        "Material": ("PROP", "Surface"),
        "Blueprint": ("ARCHITECTURE", "Logic"),
        "Texture": ("PROP", "Detail"),
    }

    STYLE_KEYWORDS = {
        "fantasy": ("magic", "rune", "dragon", "elf"),
        "sci-fi": ("laser", "cyber", "neo", "robot"),
        "medieval": ("castle", "stone", "knight", "village"),
        "realistic": ("photogrammetry", "scan", "real", "nature"),
    }

    BIOME_KEYWORDS = {
        "forest": ("tree", "leaf", "moss", "forest"),
        "desert": ("sand", "dune", "cactus"),
        "mountain": ("rock", "stone", "cliff"),
        "tundra": ("snow", "ice", "frost"),
    }

    def classify_asset_semantics(self, asset_data: AssetData) -> Classification:
        """Berechnet eine Klassifikation inklusive Tags, Stil und Biomen."""
        tags = self._derive_tags(asset_data)
        style = self.detect_asset_style(asset_data)
        biomes = [
            biome
            for biome, keywords in self.BIOME_KEYWORDS.items()
            if any(keyword in asset_data.asset_path.stem.lower() for keyword in keywords)
        ]
        if not biomes:
            biomes = ["generic"]
        primary_category, sub_category = self.CATEGORY_BY_TYPE.get(
            asset_data.asset_type,
            ("PROP", "Generic"),
        )
        technical = self._build_technical_properties(asset_data)
        classification = Classification(
            asset_path=asset_data.asset_path,
            primary_category=primary_category,
            sub_category=sub_category,
            tags=tags,
            style=style,
            biomes=biomes,
            technical=technical,
        )
        asset_data.semantic_tags = tags
        return classification

    def detect_asset_style(self, asset: AssetData) -> str:
        """Schätzt den visuellen Stil anhand einfacher Schlagworte."""
        name = asset.asset_path.stem.lower()
        for style, keywords in self.STYLE_KEYWORDS.items():
            if any(keyword in name for keyword in keywords):
                return style
        return "realistic"

    def suggest_usage_context(self, asset: AssetData) -> List[str]:
        """Leitet Nutzungskontexte aus Tags, Dateinamen und Größe ab."""
        contexts: List[str] = []
        if asset.asset_type == "StaticMesh":
            contexts.append("Level Dressing")
        if asset.metadata.vertex_count > 200_000:
            contexts.append("Hero Asset")
        if asset.metadata.vertex_count < 20_000:
            contexts.append("Background Prop")
        if "tree" in asset.asset_path.stem.lower():
            contexts.append("Vegetation Cluster")
        return contexts or ["Generic Usage"]

    def calculate_biome_compatibility(self, asset: AssetData) -> Dict[str, float]:
        """Bewertet, wie gut ein Asset zu definierten Biomen passt."""
        name = asset.asset_path.stem.lower()
        scores: Dict[str, float] = {}
        for biome, keywords in self.BIOME_KEYWORDS.items():
            score = sum(1 for keyword in keywords if keyword in name) / len(keywords)
            scores[biome] = round(score, 2)
        scores["generic"] = 0.5
        return scores

    def _build_technical_properties(self, asset_data: AssetData) -> Dict[str, object]:
        """Leitet einfache technische Eigenschaften aus Metadaten ab."""
        polycount = int(asset_data.metadata.vertex_count)
        collision = "complex" if polycount > 100_000 else "simple"
        lod = polycount > 40_000
        material_count = int(asset_data.metadata.material_slots)
        texture_resolution = self._estimate_texture_resolution(asset_data.metadata.file_size)
        return {
            "polycount": polycount,
            "texture_resolution": texture_resolution,
            "collision": collision,
            "lod": lod,
            "material_count": material_count,
        }

    @staticmethod
    def _estimate_texture_resolution(file_size_bytes: int) -> List[int]:
        """Schätzt eine sinnvolle Texturauflösung basierend auf Dateigröße."""
        size_kb = max(1, file_size_bytes // 1024)
        if size_kb > 16_384:
            return [4096, 4096]
        if size_kb > 8_192:
            return [3072, 3072]
        if size_kb > 4_096:
            return [2048, 2048]
        if size_kb > 2_048:
            return [1024, 1024]
        if size_kb > 1_024:
            return [512, 512]
        return [256, 256]

    def _derive_tags(self, asset_data: AssetData) -> List[str]:
        """Private Heuristik zur Tag-Generierung."""
        tags = {asset_data.asset_type.lower()}
        name_parts = asset_data.asset_path.stem.replace("_", " ").split()
        tags.update(part.lower() for part in name_parts if len(part) > 2)
        size = asset_data.metadata.bounds
        if size["z"] > 400:
            tags.add("tall")
        if asset_data.metadata.vertex_count > 100_000:
            tags.add("high-detail")
        return sorted(tags)

### src\auto_pcg\core\asset_scanner.py
"""Implementierung des Asset Scanners für Unreal-Projekte."""

from __future__ import annotations

import concurrent.futures
import hashlib
import json
import logging
import os
from pathlib import Path
from typing import Dict, Iterable, List, Optional, TYPE_CHECKING

from auto_pcg.models.schemas import AssetData, AssetMetadata

if TYPE_CHECKING:  # pragma: no cover - nur für Typprüfungen relevant
    from auto_pcg.data.asset_database import AssetDatabase


LOGGER = logging.getLogger(__name__)


class AssetScanner:
    """Findet Assets im Projektverzeichnis und generiert Metadaten."""

    SUPPORTED_EXTENSIONS = {
        ".uasset": "Blueprint",
        ".umap": "Blueprint",
        ".fbx": "StaticMesh",
        ".obj": "StaticMesh",
        ".gltf": "StaticMesh",
        ".glb": "StaticMesh",
        ".png": "Texture",
        ".jpg": "Texture",
        ".tga": "Texture",
        ".hdr": "Texture",
        ".material": "Material",
        ".mtl": "Material",
    }

    def __init__(self, project_root: Path, database: Optional["AssetDatabase"] = None) -> None:
        self.project_root = Path(project_root)
        self.database = database

    def scan_project_assets(self, limit: Optional[int] = None) -> List[AssetData]:
        """Durchläuft das Projektverzeichnis und liefert alle gefundenen Assets.

        Args:
            limit: Optional maximale Anzahl an Assets, bevor der Scan abgebrochen wird.
        """
        if limit is not None and limit <= 0:
            return []

        asset_paths: List[Path] = []
        for asset_path in self._iter_asset_files():
            asset_paths.append(asset_path)
            if limit is not None and len(asset_paths) >= limit:
                break

        if not asset_paths:
            return []

        max_workers = min(32, max(1, (os.cpu_count() or 4) * 2))
        assets: List[AssetData] = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            for asset in executor.map(self._safe_build_asset, asset_paths):
                if not asset:
                    continue
                assets.append(asset)
                if self.database:
                    self.database.store_asset(asset)
        return assets

    def get_asset_metadata(self, asset_path: Path) -> AssetMetadata:
        """Leitet einfache Metadaten aus Dateigröße und Dateinamen ab."""
        stat = asset_path.stat()
        file_size = stat.st_size
        complexity_factor = max(file_size // 1024, 1)
        bounds = {
            "x": float(min(500, complexity_factor % 200 + 50)),
            "y": float(min(500, (complexity_factor * 2) % 200 + 50)),
            "z": float(min(500, (complexity_factor * 3) % 200 + 50)),
        }
        vertex_count = int(min(500_000, complexity_factor * 12))
        material_slots = max(1, min(8, len(asset_path.stem) % 5 + 1))
        return AssetMetadata(
            bounds=bounds,
            vertex_count=vertex_count,
            material_slots=material_slots,
            file_size=file_size,
        )

    def generate_asset_thumbnails(self, asset_paths: Optional[Iterable[Path]] = None) -> Dict[str, str]:
        """Stub für die Thumbnail-Erzeugung, erzeugt derzeit Dateiplatzhalter."""
        thumbnail_map: Dict[str, str] = {}
        paths = asset_paths or (asset.asset_path for asset in self.database.all_assets()) if self.database else []
        for path in paths:
            thumbnail_name = path.with_suffix(".thumbnail.png").name
            thumbnail_map[str(path)] = f"generated/{thumbnail_name}"
        return thumbnail_map

    def export_asset_database(self, destination: Optional[Path] = None) -> str:
        """Exportiert die aktuelle Asset-Datenbank als JSON."""
        if self.database:
            serialized = self.database.to_json()
        else:
            assets = [asset.to_dict() for asset in self.scan_project_assets()]
            serialized = json.dumps(assets, indent=2, ensure_ascii=False)
        if destination:
            destination.write_text(serialized, encoding="utf-8")
        return serialized

    def _iter_asset_files(self) -> Iterable[Path]:
        """Hilfsmethode, die über alle unterstützten Dateien iteriert."""
        if not self.project_root.exists():
            return
        for path in self.project_root.rglob("*"):
            if path.is_file() and path.suffix.lower() in self.SUPPORTED_EXTENSIONS:
                yield path.resolve()

    @staticmethod
    def _build_asset_id(asset_path: Path) -> str:
        """Erzeugt eine reproduzierbare Asset-ID anhand des Pfads."""
        digest = hashlib.md5(str(asset_path).encode("utf-8"), usedforsecurity=False).hexdigest()
        return f"asset_{digest[:12]}"

    def _safe_build_asset(self, asset_path: Path) -> Optional[AssetData]:
        """Erzeugt ein Asset-Objekt und protokolliert Dateifehler."""
        try:
            metadata = self.get_asset_metadata(asset_path)
        except OSError as exc:  # pragma: no cover - Dateisystemfehler selten
            LOGGER.warning("Konnte Metadaten für %s nicht lesen: %s", asset_path, exc)
            return None
        asset_type = self.SUPPORTED_EXTENSIONS.get(asset_path.suffix.lower(), "Unknown")
        return AssetData(
            asset_id=self._build_asset_id(asset_path),
            asset_path=asset_path,
            asset_type=asset_type,
            metadata=metadata,
        )

### src\auto_pcg\data\__init__.py
"""Auto-PCG Modul."""

### src\auto_pcg\data\asset_database.py
"""Einfache In-Memory-Datenbank für Asset-Daten."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, Iterable, List, Sequence

from auto_pcg.models.schemas import AssetData, AssetMetadata


class AssetDatabase:
    """Verwaltet Assets, Tags und Statistiken."""

    def __init__(self) -> None:
        self._assets: Dict[str, AssetData] = {}

    def store_asset(self, asset_data: AssetData) -> None:
        """Speichert oder aktualisiert ein Asset."""
        existing = self._assets.get(asset_data.asset_id)
        if existing:
            if not asset_data.semantic_tags and existing.semantic_tags:
                asset_data.semantic_tags = existing.semantic_tags
            if not asset_data.semantic_profile and existing.semantic_profile:
                asset_data.semantic_profile = existing.semantic_profile
            if not asset_data.usage_stats:
                asset_data.usage_stats = existing.usage_stats
        self._assets[asset_data.asset_id] = asset_data

    def get_asset(self, asset_id: str) -> AssetData | None:
        """Liefert ein Asset anhand seiner ID."""
        return self._assets.get(asset_id)

    def remove_asset(self, asset_id: str) -> None:
        """Entfernt ein Asset aus der Datenbank."""
        self._assets.pop(asset_id, None)

    def query_assets_by_tags(self, tags: Sequence[str]) -> List[AssetData]:
        """Liefert alle Assets, die mindestens einen der Tags besitzen."""
        tags_lower = {tag.lower() for tag in tags}
        return [
            asset
            for asset in self._assets.values()
            if tags_lower.intersection({tag.lower() for tag in asset.semantic_tags})
        ]

    def update_usage_stats(self, asset_path: str) -> None:
        """Erhöht den Nutzungszähler eines Assets."""
        for asset in self._assets.values():
            if str(asset.asset_path) == asset_path:
                asset.usage_stats["usage_count"] = int(asset.usage_stats.get("usage_count", 0)) + 1
                asset.usage_stats["last_used"] = "auto"
                break

    def get_asset_recommendations(self, context: Sequence[str], limit: int = 5) -> List[AssetData]:
        """Empfiehlt Assets basierend auf kontextuellen Tags."""
        context_set = set(tag.lower() for tag in context)
        scored = []
        for asset in self._assets.values():
            asset_tags = {tag.lower() for tag in asset.semantic_tags}
            overlap = len(context_set.intersection(asset_tags))
            scored.append((overlap, asset))
        scored.sort(key=lambda item: item[0], reverse=True)
        return [asset for score, asset in scored if score > 0][:limit]

    def all_assets(self) -> Iterable[AssetData]:
        """Iterator über alle Assets."""
        return self._assets.values()

    def to_json(self) -> str:
        """Serialisiert die Datenbank."""
        return json.dumps([asset.to_dict() for asset in self._assets.values()], indent=2, ensure_ascii=False)

    def save_to_file(self, destination: Path) -> None:
        """Schreibt die Datenbank als JSON auf die Platte."""
        destination.write_text(self.to_json(), encoding="utf-8")

    def load_from_json(self, source: Path) -> None:
        """Lädt Assets aus einer JSON-Datei."""
        payload = json.loads(source.read_text(encoding="utf-8"))
        if not isinstance(payload, list):
            return
        self._assets.clear()
        for entry in payload:
            metadata = entry["metadata"]
            asset = AssetData(
                asset_id=entry["asset_id"],
                asset_path=Path(entry["asset_path"]),
                asset_type=entry["asset_type"],
                metadata=AssetMetadata(
                    bounds=metadata["bounds"],
                    vertex_count=metadata["vertex_count"],
                    material_slots=metadata["material_slots"],
                    file_size=metadata["file_size"],
                ),
                semantic_tags=entry.get("semantic_tags", []),
                semantic_profile=entry.get("semantic_profile", {}),
                usage_stats=entry.get("usage_stats", {}),
                relationships=entry.get("relationships", []),
            )
            self.store_asset(asset)

### src\auto_pcg\models\__init__.py
"""Auto-PCG Modul."""

### src\auto_pcg\models\schemas.py
"""Zentrale Datenmodelle für den Auto-PCG KI-Assistenten."""

from __future__ import annotations

from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Sequence


@dataclass(slots=True)
class AssetMetadata:
    """Einfaches Metadatenmodell, das grundlegende Asset-Eigenschaften enthält."""

    bounds: Dict[str, float]
    vertex_count: int
    material_slots: int
    file_size: int


@dataclass(slots=True)
class AssetData:
    """Standardisiertes Asset-Modell, das Scanner- und Analysetaten kombiniert."""

    asset_id: str
    asset_path: Path
    asset_type: str
    metadata: AssetMetadata
    semantic_tags: List[str] = field(default_factory=list)
    semantic_profile: Dict[str, object] = field(default_factory=dict)
    usage_stats: Dict[str, object] = field(
        default_factory=lambda: {
            "usage_count": 0,
            "last_used": None,
            "user_rating": 0.0,
        }
    )
    relationships: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, object]:
        """Wandelt das Asset in ein JSON-fähiges Dict um."""
        return {
            "asset_id": self.asset_id,
            "asset_path": str(self.asset_path),
            "asset_type": self.asset_type,
            "metadata": asdict(self.metadata),
            "semantic_tags": self.semantic_tags,
            "semantic_profile": self.semantic_profile,
            "usage_stats": self.usage_stats,
            "relationships": self.relationships,
        }


@dataclass(slots=True)
class Classification:
    """Ergebnis einer semantischen Asset-Analyse."""

    asset_path: Path
    primary_category: str
    sub_category: str
    tags: List[str]
    style: str
    biomes: List[str]
    technical: Dict[str, object] = field(default_factory=dict)

    def to_profile(self) -> Dict[str, object]:
        """Wandelt die Klassifikation in ein semantisches Profil um."""
        return {
            "asset_path": str(self.asset_path),
            "primary_category": self.primary_category,
            "sub_category": self.sub_category,
            "tags": self.tags,
            "style": self.style,
            "biomes": self.biomes,
            "technical": self.technical,
        }


@dataclass(slots=True)
class PCGFilterSpec:
    """Model für Filterdefinitionen in PCG-Layern."""

    type: str
    params: Dict[str, object]


@dataclass(slots=True)
class PCGLayer:
    """Beschreibung eines PCG-Layers, der im Editor erzeugt werden soll."""

    layer_type: str
    purpose: str
    assets: List[Path]
    parameters: Dict[str, object]
    filters: List[PCGFilterSpec] = field(default_factory=list)


@dataclass(slots=True)
class PCGPlan:
    """High-Level-Bauplan für einen automatisch erzeugten Graphen."""

    description: str
    target_biome: str
    layers: Sequence[PCGLayer]


@dataclass(slots=True)
class PCGNode:
    """In-Memory-Repräsentation eines PCG-Graph-Knotens."""

    name: str
    layer_type: str
    config: Dict[str, object]
    children: List["PCGNode"] = field(default_factory=list)


@dataclass(slots=True)
class PCGGraph:
    """Ergebnis des Graph Builders mit Metadaten für Debug & Export."""

    root_nodes: List[PCGNode]
    generated_at: datetime
    description: Optional[str] = None

### src\auto_pcg\models\terrain.py
"""Datenmodelle für Heightmap-, Biome- und Material-Pipelines."""

from __future__ import annotations

from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple


@dataclass(slots=True)
class HeightmapMetadata:
    """Beschreibt grundlegende Eigenschaften einer Heightmap."""

    path: Path
    width: int
    height: int
    min_elevation: float
    max_elevation: float
    average_slope: float
    water_ratio: float

    def to_dict(self) -> Dict[str, object]:
        payload = asdict(self)
        payload["path"] = str(self.path)
        return payload


@dataclass(slots=True)
class LandscapeSettings:
    """Empfohlene Landscape-Konfiguration für UE."""

    section_size: int
    components_x: int
    components_y: int
    lod_distance: float
    performance_profile: str

    def to_dict(self) -> Dict[str, object]:
        return asdict(self)


@dataclass(slots=True)
class BiomeLayer:
    """Beschreibung eines automatisch erkannten Bioms."""

    name: str
    elevation_range: Tuple[float, float]
    slope_range: Tuple[float, float]
    water_coverage: float
    mask_path: Optional[Path] = None
    weight: float = 1.0
    transitions: Dict[str, float] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, object]:
        payload = asdict(self)
        if self.mask_path:
            payload["mask_path"] = str(self.mask_path)
        return payload


@dataclass(slots=True)
class HeightmapAnalysisResult:
    """Aggregiertes Analyseergebnis inklusive Biome und Settings."""

    metadata: HeightmapMetadata
    biomes: List[BiomeLayer]
    landscape_settings: LandscapeSettings
    scale: Tuple[float, float, float]
    notes: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, object]:
        return {
            "metadata": self.metadata.to_dict(),
            "biomes": [biome.to_dict() for biome in self.biomes],
            "landscape_settings": self.landscape_settings.to_dict(),
            "scale": list(self.scale),
            "notes": self.notes,
        }


@dataclass(slots=True)
class MaterialLayerConfig:
    """Konfiguration einer Material-Layer-Kombination."""

    biome: str
    texture_set: Dict[str, str]
    tiling: float
    blending_rules: Dict[str, float]
    exposed_parameters: Dict[str, float]

    def to_dict(self) -> Dict[str, object]:
        return asdict(self)


@dataclass(slots=True)
class MaterialBlueprint:
    """Kompletter Material-Blueprint, bereit für UE Master Materials."""

    layers: List[MaterialLayerConfig]
    global_parameters: Dict[str, float]
    performance_notes: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, object]:
        return {
            "layers": [layer.to_dict() for layer in self.layers],
            "global_parameters": self.global_parameters,
            "performance_notes": self.performance_notes,
        }


@dataclass(slots=True)
class LandscapeLayerMask:
    """Beschreibt eine zu erzeugende Layer-Maske inkl. Übergangs-Infos."""

    biome: str
    mask_path: Optional[Path]
    softness: float
    recommended_order: int

    def to_dict(self) -> Dict[str, object]:
        payload = asdict(self)
        if self.mask_path:
            payload["mask_path"] = str(self.mask_path)
        return payload


@dataclass(slots=True)
class LandscapeLayerPlan:
    """Plan für automatisches Landscape-Painting."""

    masks: List[LandscapeLayerMask]
    adaptive_rules: Dict[str, float]

    def to_dict(self) -> Dict[str, object]:
        return {
            "masks": [mask.to_dict() for mask in self.masks],
            "adaptive_rules": self.adaptive_rules,
        }

### src\auto_pcg\pcg\__init__.py
"""Auto-PCG Modul."""

### src\auto_pcg\pcg\graph_builder.py
"""Erstellt PCG-Graphen aus abstrakten Plänen."""

from __future__ import annotations

from datetime import datetime
from pathlib import Path
from typing import Iterable, List, Sequence

from auto_pcg.models.schemas import PCGFilterSpec, PCGGraph, PCGLayer, PCGNode, PCGPlan


class PCGBuilder:
    """Konvertiert PCG-Pläne in eine Graphstruktur, die später nach UE exportiert werden kann."""

    def create_pcg_graph_from_plan(self, plan: PCGPlan) -> PCGGraph:
        """Überführt alle Layer in PCG-Knoten."""
        nodes = [self._layer_to_node(layer, index) for index, layer in enumerate(plan.layers)]
        return PCGGraph(root_nodes=nodes, generated_at=datetime.utcnow(), description=plan.description)

    def build_surface_layer(self, spec: PCGLayer) -> PCGNode:
        """Erzeugt einen SURFACE-Knoten inkl. Parameter."""
        config = {
            "material_asset": str(spec.assets[0]) if spec.assets else None,
            **spec.parameters,
        }
        return PCGNode(name="SurfaceLayer", layer_type="SURFACE", config=config)

    def build_scatter_layer(self, assets: Sequence[Path], density: float) -> PCGNode:
        """Erzeugt einen SCATTER-Knoten basierend auf Assets und Dichte."""
        config = {
            "assets": [str(asset) for asset in assets],
            "density": density,
        }
        return PCGNode(name="ScatterLayer", layer_type="SCATTER", config=config)

    def apply_filters(self, node: PCGNode, filter_spec: Iterable[PCGFilterSpec]) -> PCGNode:
        """Fügt Filterinformationen in den Knotenkonfig an."""
        node.config.setdefault("filters", [])
        for spec in filter_spec:
            entry = {"type": spec.type, **spec.params}
            node.config["filters"].append(entry)
        return node

    # Hilfsfunktionen -------------------------------------------------------------------------

    def _layer_to_node(self, layer: PCGLayer, index: int) -> PCGNode:
        """Wandelt einen Layer in einen Knoten mit eindeutigen Namen um."""
        if layer.layer_type == "SURFACE":
            node = self.build_surface_layer(layer)
        elif layer.layer_type == "SCATTER":
            node = self.build_scatter_layer(layer.assets, float(layer.parameters.get("density", 0.5)))
        else:
            node = PCGNode(name="GenericLayer", layer_type=layer.layer_type, config=dict(layer.parameters))
        node.name = f"{layer.layer_type}_{index}"
        return self.apply_filters(node, layer.filters)

### src\auto_pcg\pcg\ue_integration.py
"""Helpers to import Auto-PCG Graphs into Unreal via command line."""

from __future__ import annotations

import subprocess
from pathlib import Path
from typing import Optional


def run_unreal_import(
    editor_exe: Path,
    uproject: Path,
    graph_path: Path,
    script_path: Path,
    asset_folder: str,
    asset_name: Optional[str] = None,
    map_path: Optional[str] = None,
    spawn: bool = True,
) -> None:
    def _fmt_path(value: Path) -> str:
        # Unreal tolerates forward slashes even on Windows and they avoid \u escaping issues
        return str(Path(value).resolve()).replace("\\", "/")

    script_args = [
        f"--graph-json={_fmt_path(graph_path)}",
        f"--asset-folder={asset_folder}",
    ]
    if asset_name:
        script_args.append(f"--asset-name={asset_name}")
    if spawn:
        script_args.append("--spawn")
    if map_path:
        script_args.append(f"--spawn-map={map_path}")
    cmd = [
        str(editor_exe),
        str(uproject),
        "-run=pythonscript",
        f"-script={_fmt_path(script_path)}",
        "--",
        *script_args,
    ]
    subprocess.run(cmd, check=True)

### src\auto_pcg\pcg\unreal_exporter.py
"""Hilfsklasse, um PCG-Graphen als JSON für Unreal zu exportieren."""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List

from auto_pcg.models.schemas import PCGGraph, PCGNode


class UnrealPCGExporter:
    """Schreibt PCG-Graphen in ein einfaches JSON-Format für UE-Imports/Skripte."""

    def __init__(self, export_root: Path) -> None:
        self.export_root = Path(export_root)
        self.export_root.mkdir(parents=True, exist_ok=True)

    def export(self, graph: PCGGraph, file_prefix: str = "pcg_graph") -> Path:
        """Speichert den Graphen als JSON und gibt den Pfad zurück."""
        timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
        destination = self.export_root / f"{file_prefix}_{timestamp}.json"
        payload = self._graph_to_dict(graph)
        destination.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding="utf-8")
        return destination

    def _graph_to_dict(self, graph: PCGGraph) -> Dict[str, object]:
        return {
            "description": graph.description,
            "generated_at": graph.generated_at.isoformat(),
            "nodes": [self._node_to_dict(node) for node in graph.root_nodes],
        }

    def _node_to_dict(self, node: PCGNode) -> Dict[str, object]:
        return {
            "name": node.name,
            "type": node.layer_type,
            "config": node.config,
            "children": [self._node_to_dict(child) for child in node.children],
        }

### src\auto_pcg\scripts\ue_pcg_import.py
"""UE Editor Python script to build a PCG Graph + optional placement from Auto-PCG JSON."""

import argparse
import json
from pathlib import Path

import unreal


def load_json_graph(path: Path) -> dict:
    data = json.loads(path.read_text(encoding="utf-8"))
    if "graph" not in data:
        raise RuntimeError("JSON enthaelt kein 'graph'-Objekt.")
    return data["graph"]


def ensure_pcg_plugin_enabled() -> None:
    required_attrs = [
        "PCGGraph",
        "PCGGraphFactory",
        "PCGVolume",
        "PCGScatterSettings",
        "PCGSurfaceSamplerSettings",
    ]
    missing = [name for name in required_attrs if not hasattr(unreal, name)]
    if missing:
        missing_str = ", ".join(missing)
        raise RuntimeError(
            "PCG-Plugin (Procedural Content Generation Framework) ist nicht aktiviert "
            f"oder nicht installiert. Fehlende Klassen: {missing_str}"
        )


def ensure_asset_folder(package_path: str) -> None:
    if not unreal.EditorAssetLibrary.does_directory_exist(package_path):
        unreal.EditorAssetLibrary.make_directory(package_path)


def create_or_load_graph(asset_path: str) -> unreal.PCGGraph:
    if unreal.EditorAssetLibrary.does_asset_exist(asset_path):
        asset = unreal.EditorAssetLibrary.load_asset(asset_path)
        if not isinstance(asset, unreal.PCGGraph):
            raise RuntimeError(f"Asset {asset_path} existiert, ist aber kein PCGGraph.")
        return asset
    ensure_asset_folder("/".join(asset_path.split("/")[:-1]))
    factory = unreal.PCGGraphFactory()
    asset_name = asset_path.split("/")[-1]
    package_path = "/".join(asset_path.split("/")[:-1])
    graph = unreal.AssetToolsHelpers.get_asset_tools().create_asset(
        asset_name, package_path, unreal.PCGGraph, factory
    )
    return graph


def add_surface_node(graph: unreal.PCGGraph, node_data: dict, position_x: float) -> None:
    settings = unreal.PCGSurfaceSamplerSettings()
    material_path = node_data.get("config", {}).get("material_asset")
    if material_path:
        settings.surface = unreal.SoftObjectPath(material_path)
    settings.tiling = float(node_data.get("config", {}).get("tiling", 1.0))
    node = graph.add_node(settings, unreal.Vector2D(position_x, 0.0))
    node.set_node_name(node_data.get("name", "Surface"))


def add_scatter_node(graph: unreal.PCGGraph, node_data: dict, position_x: float) -> None:
    settings = unreal.PCGScatterSettings()
    config = node_data.get("config", {})
    density = config.get("density") or config.get("probability") or 0.5
    settings.point_density = float(density)
    node = graph.add_node(settings, unreal.Vector2D(position_x, 0.0))
    node.set_node_name(node_data.get("name", "Scatter"))
    assets = config.get("assets", [])
    if assets:
        node_graph = node.get_graph()
        node_graph.set_metadata_entry("AutoPCGAssets", ",".join(str(a) for a in assets))


def rebuild_graph(graph: unreal.PCGGraph, graph_spec: dict) -> None:
    graph.remove_all_nodes()
    nodes = graph_spec.get("nodes", [])
    spacing = 450.0
    for index, node_data in enumerate(nodes):
        node_type = node_data.get("type", "").upper()
        pos = index * spacing
        if node_type == "SURFACE":
            add_surface_node(graph, node_data, pos)
        else:
            add_scatter_node(graph, node_data, pos)
    graph.mark_package_dirty()


def spawn_pcg_volume(graph_asset: unreal.PCGGraph, location: unreal.Vector) -> None:
    pcg_volume_class = unreal.PCGVolume
    volume = unreal.EditorLevelLibrary.spawn_actor_from_class(pcg_volume_class, location)
    volume.set_graph(graph_asset)
    volume.get_pcg_component().generate()


def load_target_map(map_path: str) -> bool:
    result = unreal.EditorLoadingAndSavingUtils.load_map(map_path)
    if not result:
        unreal.log_error(f"Konnte Map {map_path} nicht laden. Bitte Pfad pruefen.")
    return bool(result)


def load_optional_payload(path: str | None) -> dict | None:
    if not path:
        return None
    json_path = Path(path).resolve()
    if not json_path.exists():
        unreal.log_warning(f"Konnte Datei {json_path} nicht finden.")
        return None
    try:
        return json.loads(json_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        unreal.log_warning(f"JSON konnte nicht geladen werden ({json_path}): {exc}")
        return None


def attach_metadata(asset: unreal.Object, key: str, payload: dict | None) -> None:
    if not payload:
        return
    try:
        serialized = json.dumps(payload)
        unreal.EditorAssetLibrary.set_metadata_tag(asset, key, serialized)
    except Exception as exc:  # pragma: no cover - Editor API spezifisch
        unreal.log_warning(f"Konnte Metadata {key} nicht setzen: {exc}")


def load_optional_payload(path: str | None) -> dict | None:
    if not path:
        return None
    json_path = Path(path).resolve()
    if not json_path.exists():
        unreal.log_warning(f"Konnte Datei {json_path} nicht finden.")
        return None
    try:
        return json.loads(json_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        unreal.log_warning(f"JSON konnte nicht geladen werden ({json_path}): {exc}")
        return None


def attach_metadata(asset: unreal.Object, key: str, payload: dict | None) -> None:
    if not payload:
        return
    try:
        serialized = json.dumps(payload)
        unreal.EditorAssetLibrary.set_metadata_tag(asset, key, serialized)
    except Exception as exc:  # pragma: no cover - Editor API spezifisch
        unreal.log_warning(f"Konnte Metadata {key} nicht setzen: {exc}")


def load_texture(texture_path: str | None) -> unreal.Texture | None:
    if not texture_path:
        return None
    try:
        asset = unreal.EditorAssetLibrary.load_asset(texture_path)
    except Exception:  # pragma: no cover - Editor API spezifisch
        asset = None
    if not asset:
        unreal.log_warning(f"Textur {texture_path} konnte nicht geladen werden.")
    return asset


def build_material_from_blueprint(
    blueprint: dict,
    asset_folder: str,
    asset_name: str,
) -> unreal.Material | None:
    layers = blueprint.get("layers") if isinstance(blueprint, dict) else None
    if not layers:
        return None
    package_path = f"{asset_folder.rstrip('/')}/Materials"
    ensure_asset_folder(package_path)
    material_name = f"{asset_name}_LandscapeMat"
    asset_path = f"{package_path}/{material_name}"
    material = unreal.EditorAssetLibrary.load_asset(asset_path)
    if not material:
        factory = unreal.MaterialFactoryNew()
        material = unreal.AssetToolsHelpers.get_asset_tools().create_asset(
            material_name,
            package_path,
            unreal.Material,
            factory,
        )
    if not material:
        return None
    material.set_editor_property("use_with_landscape", True)
    mel = unreal.MaterialEditingLibrary
    mel.delete_all_material_expressions(material)
    mel.recompile_material(material)

    prev_expr = None
    pos_y = 0
    for layer in layers:
        if not isinstance(layer, dict):
            continue
        biome = str(layer.get("biome", f"Layer_{pos_y}"))
        texture_set = layer.get("texture_set", {})
        tiling = float(layer.get("tiling", 1.0))
        tex = load_texture(texture_set.get("albedo"))

        coord = mel.create_material_expression(
            material,
            unreal.MaterialExpressionTextureCoordinate,
            node_pos_x=-600,
            node_pos_y=pos_y,
        )
        coord.set_editor_property("utiling", tiling)
        coord.set_editor_property("vtiling", tiling)

        if tex:
            texture_expr = mel.create_material_expression(
                material,
                unreal.MaterialExpressionTextureSampleParameter2D,
                node_pos_x=-350,
                node_pos_y=pos_y,
            )
            texture_expr.texture = tex
            texture_expr.set_editor_property("parameter_name", f"{biome}_Albedo")
            mel.connect_material_expressions(coord, "", texture_expr, "Coordinates")
            color_expr = texture_expr
        else:
            const_expr = mel.create_material_expression(
                material,
                unreal.MaterialExpressionConstant3Vector,
                node_pos_x=-350,
                node_pos_y=pos_y,
            )
            color_expr = const_expr

        layer_sample = mel.create_material_expression(
            material,
            unreal.MaterialExpressionLandscapeLayerSample,
            node_pos_x=-150,
            node_pos_y=pos_y,
        )
        layer_sample.set_editor_property("parameter_name", biome)

        multiply = mel.create_material_expression(
            material,
            unreal.MaterialExpressionMultiply,
            node_pos_x=100,
            node_pos_y=pos_y,
        )
        mel.connect_material_expressions(layer_sample, "", multiply, "A")
        mel.connect_material_expressions(color_expr, "", multiply, "B")

        if tex and "normal" in texture_set:
            normal_tex = load_texture(texture_set.get("normal"))
            if normal_tex:
                normal_expr = mel.create_material_expression(
                    material,
                    unreal.MaterialExpressionTextureSampleParameter2D,
                    node_pos_x=-350,
                    node_pos_y=pos_y + 200,
                )
                normal_expr.texture = normal_tex
                normal_expr.set_editor_property("parameter_name", f"{biome}_Normal")
                mel.connect_material_property(
                    normal_expr,
                    "",
                    unreal.MaterialProperty.MP_NORMAL,
                )

        if prev_expr is None:
            prev_expr = multiply
        else:
            add_expr = mel.create_material_expression(
                material,
                unreal.MaterialExpressionAdd,
                node_pos_x=350,
                node_pos_y=pos_y,
            )
            mel.connect_material_expressions(prev_expr, "", add_expr, "A")
            mel.connect_material_expressions(multiply, "", add_expr, "B")
            prev_expr = add_expr
        pos_y += 300

    if prev_expr:
        mel.connect_material_property(prev_expr, "", unreal.MaterialProperty.MP_BASE_COLOR)

    # Roughness aus globalen Parametern ableiten
    globals_payload = blueprint.get("global_parameters", {}) if isinstance(blueprint, dict) else {}
    roughness_value = float(globals_payload.get("roughness", globals_payload.get("roughness_min", 0.5)))
    const = mel.create_material_expression(
        material,
        unreal.MaterialExpressionConstant,
        node_pos_x=200,
        node_pos_y=pos_y + 50,
    )
    const.set_editor_property("r", roughness_value)
    mel.connect_material_property(const, "", unreal.MaterialProperty.MP_ROUGHNESS)
    mel.layout_material_expressions(material)
    mel.recompile_material(material)
    unreal.EditorAssetLibrary.save_loaded_asset(material)
    return material


def assign_material_to_landscapes(material: unreal.Material) -> None:
    if not material:
        return
    actors = unreal.EditorLevelLibrary.get_all_level_actors()
    landscapes = [
        actor
        for actor in actors
        if isinstance(actor, (unreal.LandscapeProxy, unreal.Landscape))
    ]
    if not landscapes:
        unreal.log_warning("Keine Landscape-Actors gefunden, Material wird nicht angewandt.")
        return
    for landscape in landscapes:
        try:
            landscape.set_editor_property("landscape_material", material)
            landscape.post_edit_change()
        except Exception as exc:  # pragma: no cover - Editor API spezifisch
            unreal.log_warning(f"Material konnte nicht auf {landscape.get_name()} angewandt werden: {exc}")


def create_layer_info(layer_name: str, asset_folder: str) -> unreal.LandscapeLayerInfoObject | None:
    layer_dir = f"{asset_folder.rstrip('/')}/LayerInfo"
    ensure_asset_folder(layer_dir)
    info_name = f"{layer_name}_LayerInfo"
    asset_path = f"{layer_dir}/{info_name}"
    info = unreal.EditorAssetLibrary.load_asset(asset_path)
    if info:
        return info
    factory = unreal.LandscapeLayerInfoObjectFactory()
    info = unreal.AssetToolsHelpers.get_asset_tools().create_asset(
        info_name,
        layer_dir,
        unreal.LandscapeLayerInfoObject,
        factory,
    )
    if info:
        info.set_editor_property("layer_name", layer_name)
        info.set_editor_property("hardness", 0.5)
        unreal.EditorAssetLibrary.save_loaded_asset(info)
    return info


def apply_layer_plan(layer_plan: dict | None, asset_folder: str) -> None:
    if not layer_plan:
        return
    masks = layer_plan.get("masks")
    if not isinstance(masks, list):
        return
    layer_infos = []
    for entry in masks:
        if not isinstance(entry, dict):
            continue
        biome = str(entry.get("biome", "Layer"))
        info = create_layer_info(biome, asset_folder)
        if info:
            layer_infos.append(info)
    if not layer_infos:
        return
    actors = unreal.EditorLevelLibrary.get_all_level_actors()
    landscapes = [
        actor
        for actor in actors
        if isinstance(actor, (unreal.LandscapeProxy, unreal.Landscape))
    ]
    if not landscapes:
        unreal.log_warning("Keine Landscape-Actors gefunden, Layer-Plan kann nicht angewandt werden.")
        return
    subsystem = unreal.get_editor_subsystem(unreal.LandscapeEditorSubsystem)
    for landscape in landscapes:
        for info in layer_infos:
            try:
                if hasattr(subsystem, "set_layer_info"):
                    subsystem.set_layer_info(landscape, info.get_editor_property("layer_name"), info)
                else:
                    landscape.set_layer_info(info.get_editor_property("layer_name"), info)  # type: ignore[attr-defined]
            except Exception as exc:  # pragma: no cover - Editor API spezifisch
                unreal.log_warning(f"Konnte LayerInfo auf {landscape.get_name()} nicht anwenden: {exc}")


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--graph-json", required=True)
    parser.add_argument("--asset-folder", default="/Game/AutoPCG")
    parser.add_argument("--asset-name", default=None)
    parser.add_argument("--spawn", action="store_true")
    parser.add_argument("--spawn-map", default=None)
    parser.add_argument("--material-json", default=None, help="Optionaler Material-Blueprint (JSON)")
    parser.add_argument("--layer-plan", default=None, help="Optionaler Layer-Plan (JSON)")
    parser.add_argument("--heightmap-json", default=None, help="Optionales Heightmap-Analyse-JSON")
    args = parser.parse_args()

    ensure_pcg_plugin_enabled()

    json_path = Path(args.graph_json).resolve()
    graph_spec = load_json_graph(json_path)

    asset_name = args.asset_name or json_path.stem
    asset_path = f"{args.asset_folder.rstrip('/')}/{asset_name}"

    map_ready = True
    if args.spawn_map:
        map_ready = load_target_map(args.spawn_map)

    graph_asset = create_or_load_graph(asset_path)
    rebuild_graph(graph_asset, graph_spec)
    unreal.EditorAssetLibrary.save_loaded_asset(graph_asset)

    material_payload = load_optional_payload(args.material_json)
    material_asset = None
    if material_payload:
        material_asset = build_material_from_blueprint(material_payload, args.asset_folder, asset_name)
        if material_asset:
            assign_material_to_landscapes(material_asset)

    layer_payload = load_optional_payload(args.layer_plan)
    if layer_payload:
        apply_layer_plan(layer_payload, args.asset_folder)

    heightmap_payload = load_optional_payload(args.heightmap_json)

    # Metadaten fr sptere Inspektionen beibehalten
    attach_metadata(graph_asset, "AutoPCG_MaterialBlueprint", material_payload)
    attach_metadata(graph_asset, "AutoPCG_LandscapeLayers", layer_payload)
    attach_metadata(graph_asset, "AutoPCG_HeightmapAnalysis", heightmap_payload)

    should_spawn = bool(args.spawn and map_ready)
    if args.spawn and not should_spawn:
        unreal.log_warning("PCG-Volume wird nicht gespawnt, da die Map nicht geladen werden konnte.")

    if should_spawn:
        spawn_pcg_volume(graph_asset, unreal.Vector(0.0, 0.0, 0.0))

    unreal.log(f"Auto-PCG Graph importiert nach {asset_path}")


if __name__ == "__main__":
    main()

### src\auto_pcg\services\__init__.py
"""Auto-PCG Modul."""

### src\auto_pcg\services\pcg_service.py
"""High-Level-Service, der Scanner, KI und PCG-Builder orchestriert."""

from __future__ import annotations

import os
import logging
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence

from auto_pcg.ai.llm_manager import LLMManager
from auto_pcg.ai.prompt_engine import PromptEngine
from auto_pcg.core.asset_analyzer import AssetAnalyzer
from auto_pcg.core.asset_scanner import AssetScanner
from auto_pcg.data.asset_database import AssetDatabase
from auto_pcg.models.schemas import AssetData, Classification, PCGGraph, PCGPlan
from auto_pcg.models.terrain import (
    HeightmapAnalysisResult,
    LandscapeLayerMask,
    LandscapeLayerPlan,
    MaterialBlueprint,
    MaterialLayerConfig,
)
from auto_pcg.pcg.graph_builder import PCGBuilder
from auto_pcg.pcg.unreal_exporter import UnrealPCGExporter
from auto_pcg.pcg.ue_integration import run_unreal_import
from auto_pcg.terrain import HeightmapProcessor, LayerPainter, MaterialPlanner


LOGGER = logging.getLogger(__name__)


class AutoPCGService:
    """Öffnet eine einfache Python-API für das Auto-PCG-System."""

    def __init__(
        self,
        project_root: Path,
        *,
        max_assets: Optional[int] = None,
        prefer_heuristics: bool = False,
        classification_batch_size: Optional[int] = None,
        ollama_url: Optional[str] = None,
        ollama_model: Optional[str] = None,
        ollama_timeout: Optional[float] = None,
        use_local_model: bool = True,
        export_directory: Optional[Path] = None,
        heightmap: Optional[Path] = None,
        performance_profile: str = "desktop",
        target_style: str = "realistic",
        auto_layer_paint: bool = True,
        ue_editor: Optional[Path] = None,
        ue_map: Optional[str] = None,
        ue_asset_folder: str = "/Game/AutoPCG",
        ue_spawn: bool = True,
    ) -> None:
        self.database = AssetDatabase()
        self._has_scanned = False
        self._max_assets = max_assets
        self._prefer_heuristics = prefer_heuristics
        self._project_root = Path(project_root)
        self._export_directory = export_directory
        self._heightmap_path = Path(heightmap).resolve() if heightmap else None
        self._performance_profile = performance_profile
        self._target_style = target_style
        self._auto_layer_paint = auto_layer_paint
        self._ue_editor = Path(ue_editor) if ue_editor else None
        self._ue_map = ue_map
        self._ue_asset_folder = ue_asset_folder
        self._ue_spawn = ue_spawn
        self._ue_script = Path(__file__).resolve().parents[1] / "scripts" / "ue_pcg_import.py"
        self.cache_path = self._resolve_cache_path(project_root)
        if self.cache_path and self.cache_path.exists():
            try:
                self.database.load_from_json(self.cache_path)
                LOGGER.info("Geladene Asset-Datenbank: %s", self.cache_path)
            except Exception as exc:  # pragma: no cover - Dateifehler
                LOGGER.warning("Konnte Asset-Cache nicht laden (%s): %s", self.cache_path, exc)
        self.scanner = AssetScanner(project_root, self.database)
        self.analyzer = AssetAnalyzer()
        self.prompt_engine = PromptEngine()
        self.llm_manager = LLMManager(
            prompt_engine=self.prompt_engine,
            local_model_path=self._resolve_local_model_path() if use_local_model else None,
            classification_batch_size=classification_batch_size,
            base_url=ollama_url or "http://localhost:11434",
            model=ollama_model or "llama3",
            timeout=ollama_timeout or 10.0,
        )
        self.graph_builder = PCGBuilder()
        self.heightmap_processor = HeightmapProcessor()
        self.material_planner = MaterialPlanner()
        self.layer_painter = LayerPainter()
        self.heightmap_analysis: Optional[HeightmapAnalysisResult] = None
        self.material_blueprint: Optional[MaterialBlueprint] = None
        self.layer_plan: Optional[LandscapeLayerPlan] = None
        self._exporter = (
            UnrealPCGExporter(self._export_directory) if self._export_directory else None
        )
        self._uproject_path = self._find_uproject(self._project_root)
        if not self.llm_manager._local_client:
            self.llm_manager.setup_ollama_connection()

    def scan_and_classify_assets(self) -> List[AssetData]:
        """Führt den Asset-Scan durch und klassifiziert jedes Asset."""
        assets = self.scanner.scan_project_assets(limit=self._max_assets)
        if self._max_assets is not None:
            LOGGER.info(
                "Asset-Scan auf maximal %s Dateien begrenzt (gefunden: %s).",
                self._max_assets,
                len(assets),
            )
        assets_to_classify: List[AssetData] = []
        for asset in assets:
            cached = self.database.get_asset(asset.asset_id)
            if cached and cached.semantic_tags:
                asset.semantic_tags = cached.semantic_tags
                asset.semantic_profile = cached.semantic_profile
                self.database.store_asset(asset)
            else:
                assets_to_classify.append(asset)

        if assets_to_classify:
            if self._prefer_heuristics:
                LOGGER.info(
                    "Überspringe LLM-Klassifikation und nutze heuristischen Schnellmodus (%s Assets).",
                    len(assets_to_classify),
                )
                for asset in assets_to_classify:
                    classification = self.analyzer.classify_asset_semantics(asset)
                    self._apply_classification(asset, classification)
                    self.database.store_asset(asset)
            else:
                classifications = self.llm_manager.send_classification_request(assets_to_classify)
                by_path = {
                    classification.asset_path.resolve(): classification
                    for classification in classifications
                }
                for asset in assets_to_classify:
                    classification = by_path.get(asset.asset_path.resolve())
                    if classification:
                        self._apply_classification(asset, classification)
                    self.database.store_asset(asset)

        self._persist_database()
        self._has_scanned = True
        return assets

    def generate_pcg_plan(self, user_prompt: str, asset_subset: Sequence[AssetData] | None = None) -> PCGPlan:
        """Erstellt einen PCG-Plan für den angegebenen Textbefehl."""
        if not self._has_scanned:
            LOGGER.info("Starte automatischen Asset-Scan vor der Planerstellung.")
            self.scan_and_classify_assets()
        context_assets = list(asset_subset or self._choose_context_assets(user_prompt))
        if not context_assets:
            context_assets = list(self.database.all_assets())
        return self.llm_manager.send_pcg_generation_request(user_prompt, context_assets)

    def build_graph_for_prompt(self, user_prompt: str) -> PCGGraph:
        """Kompletter Workflow von Text zu PCG-Graph."""
        if not self._has_scanned:
            LOGGER.info("Assets wurden noch nicht gescannt – führe Scan jetzt aus.")
            self.scan_and_classify_assets()
        plan = self.generate_pcg_plan(user_prompt)
        graph = self.graph_builder.create_pcg_graph_from_plan(plan)
        export_path = None
        if self._exporter:
            export_path = self._exporter.export(graph)
            LOGGER.info("PCG-Graph nach %s exportiert.", export_path)
        if self._ue_editor and export_path:
            self._trigger_unreal_import(export_path)
        return graph

    def run_full_pipeline(self, user_prompt: str) -> Dict[str, object]:
        """Führt Heightmap-, Asset-, PCG- und Material-Schritte automatisch aus."""
        analysis = self._ensure_heightmap_analysis()
        assets = self.scan_and_classify_assets()
        graph = self.build_graph_for_prompt(user_prompt)
        blueprint = self._ensure_material_blueprint()
        layer_plan = self._ensure_layer_plan()
        return {
            "graph": graph,
            "assets": assets,
            "heightmap_analysis": analysis,
            "material_blueprint": blueprint,
            "layer_plan": layer_plan,
        }

    # Private Hilfen ----------------------------------------------------------------------------

    def _ensure_heightmap_analysis(self) -> Optional[HeightmapAnalysisResult]:
        if self.heightmap_analysis or not self._heightmap_path:
            return self.heightmap_analysis
        try:
            analysis = self.heightmap_processor.process_heightmap(
                self._heightmap_path,
                target_style=self._target_style,
                performance_profile=self._performance_profile,
            )
        except FileNotFoundError as exc:
            LOGGER.warning("Heightmap konnte nicht verarbeitet werden: %s", exc)
            return None
        strategy = self.llm_manager.plan_heightmap_strategy(analysis)
        if strategy:
            self._apply_heightmap_strategy(analysis, strategy)
        self.heightmap_analysis = analysis
        return analysis

    def _apply_heightmap_strategy(self, analysis: HeightmapAnalysisResult, strategy: Dict[str, object]) -> None:
        notes = strategy.get("notes")
        if isinstance(notes, list):
            analysis.notes.extend(str(entry) for entry in notes)
        scale = strategy.get("scale")
        if isinstance(scale, (list, tuple)) and len(scale) == 3:
            analysis.scale = tuple(float(value) for value in scale)  # type: ignore[assignment]
        settings = strategy.get("landscape_settings")
        if isinstance(settings, dict):
            cast_map = {
                "section_size": int,
                "components_x": int,
                "components_y": int,
                "lod_distance": float,
            }
            for field, caster in cast_map.items():
                if field in settings:
                    try:
                        setattr(analysis.landscape_settings, field, caster(settings[field]))
                    except (TypeError, ValueError):
                        continue
        recommended = strategy.get("recommended_biomes")
        if isinstance(recommended, list):
            for biome, suggestion in zip(analysis.biomes, recommended):
                if not isinstance(suggestion, dict):
                    continue
                biome.name = str(suggestion.get("name", biome.name))
                if "elevation_range" in suggestion:
                    values = suggestion["elevation_range"]
                    if isinstance(values, (list, tuple)) and len(values) == 2:
                        biome.elevation_range = (float(values[0]), float(values[1]))
                if "slope_range" in suggestion:
                    values = suggestion["slope_range"]
                    if isinstance(values, (list, tuple)) and len(values) == 2:
                        biome.slope_range = (float(values[0]), float(values[1]))

    def _ensure_material_blueprint(self) -> Optional[MaterialBlueprint]:
        if self.material_blueprint:
            return self.material_blueprint
        analysis = self._ensure_heightmap_analysis()
        if not analysis:
            return None
        blueprint = self.material_planner.build_blueprint(
            analysis,
            target_style=self._target_style,
            performance_profile=self._performance_profile,
        )
        suggestion = self.llm_manager.plan_material_blueprint(analysis, blueprint)
        if suggestion:
            blueprint = self._merge_material_blueprint(blueprint, suggestion)
        self.material_blueprint = blueprint
        return blueprint

    def _merge_material_blueprint(
        self,
        blueprint: MaterialBlueprint,
        payload: Dict[str, object],
    ) -> MaterialBlueprint:
        layers_payload = payload.get("layers")
        if isinstance(layers_payload, list) and layers_payload:
            new_layers: List[MaterialLayerConfig] = []
            for entry in layers_payload:
                layer = self._layer_config_from_payload(entry)
                if layer:
                    new_layers.append(layer)
            if new_layers:
                blueprint.layers = new_layers
        global_parameters = payload.get("global_parameters")
        if isinstance(global_parameters, dict):
            blueprint.global_parameters.update(
                {str(key): float(value) for key, value in global_parameters.items() if isinstance(value, (int, float))}
            )
        notes = payload.get("performance_notes")
        if isinstance(notes, list):
            blueprint.performance_notes.extend(str(note) for note in notes)
        return blueprint

    def _layer_config_from_payload(self, entry: object) -> Optional[MaterialLayerConfig]:
        if not isinstance(entry, dict):
            return None
        biome = str(entry.get("biome") or entry.get("name") or "biome")
        texture_set = entry.get("texture_set") or entry.get("textures") or {}
        if not isinstance(texture_set, dict):
            texture_set = {}
        tiling = float(entry.get("tiling", 1.0))
        blending = entry.get("blending_rules") or entry.get("blending") or {}
        if not isinstance(blending, dict):
            blending = {}
        parameters = entry.get("exposed_parameters") or entry.get("parameters") or {}
        if not isinstance(parameters, dict):
            parameters = {}
        return MaterialLayerConfig(
            biome=biome,
            texture_set={str(key): str(value) for key, value in texture_set.items()},
            tiling=tiling,
            blending_rules={str(key): float(value) for key, value in blending.items() if isinstance(value, (int, float))},
            exposed_parameters={
                str(key): float(value) for key, value in parameters.items() if isinstance(value, (int, float))
            },
        )

    def _ensure_layer_plan(self) -> Optional[LandscapeLayerPlan]:
        if self.layer_plan or not self._auto_layer_paint:
            return self.layer_plan
        analysis = self._ensure_heightmap_analysis()
        if not analysis:
            return None
        plan = self.layer_painter.build_layer_plan(
            analysis,
            output_dir=self._export_directory,
        )
        suggestion = self.llm_manager.plan_layer_paint(plan)
        if suggestion:
            plan = self._merge_layer_plan(plan, suggestion)
        self.layer_plan = plan
        return plan

    def _merge_layer_plan(
        self,
        plan: LandscapeLayerPlan,
        payload: Dict[str, object],
    ) -> LandscapeLayerPlan:
        masks = payload.get("masks")
        if isinstance(masks, list) and masks:
            new_masks: List[LandscapeLayerMask] = []
            for entry in masks:
                if not isinstance(entry, dict):
                    continue
                biome = str(entry.get("biome", "layer"))
                mask_path = entry.get("mask_path")
                path_obj = Path(mask_path) if isinstance(mask_path, str) else None
                softness = float(entry.get("softness", 0.25))
                order = int(entry.get("recommended_order", len(new_masks)))
                new_masks.append(
                    LandscapeLayerMask(
                        biome=biome,
                        mask_path=path_obj,
                        softness=softness,
                        recommended_order=order,
                    )
                )
            if new_masks:
                plan.masks = new_masks
        adaptive = payload.get("adaptive_rules")
        if isinstance(adaptive, dict):
            plan.adaptive_rules.update(
                {str(key): float(value) for key, value in adaptive.items() if isinstance(value, (int, float))}
            )
        return plan

    def _choose_context_assets(self, user_prompt: str) -> Iterable[AssetData]:
        """Nutzt einfache Keyword-Extraktion, um relevante Assets zu finden."""
        keywords = [
            token.lower()
            for token in user_prompt.replace(",", " ").split()
            if len(token) > 3
        ]
        if not keywords:
            return self.database.all_assets()
        recommendations = self.database.get_asset_recommendations(keywords, limit=5)
        if recommendations:
            return recommendations
        return self.database.all_assets()

    def _resolve_local_model_path(self) -> Optional[Path]:
        """Sucht nach einem GGUF-Modell im Projekt oder über Umgebungsvariable."""
        env_path = os.getenv("AUTO_PCG_GGUF_MODEL")
        if env_path:
            candidate = Path(env_path).expanduser()
            if candidate.exists():
                return candidate
        module_path = Path(__file__).resolve()
        repo_root = module_path.parent
        try:
            repo_root = module_path.parents[3]
        except IndexError:
            repo_root = module_path.parent
        search_roots = [
            Path.cwd(),
            repo_root,
        ]
        for root in search_roots:
            for match in root.glob("*.gguf"):
                return match
        return None

    def _resolve_cache_path(self, project_root: Path) -> Optional[Path]:
        """Bestimmt, wo die Asset-Datenbank zwischengespeichert wird."""
        env_cache = os.getenv("AUTO_PCG_CACHE")
        if env_cache:
            return Path(env_cache).expanduser()
        try:
            project_root = Path(project_root).resolve()
        except OSError:
            project_root = Path.cwd()
        return project_root / ".auto_pcg_assets.json"

    def _persist_database(self) -> None:
        """Schreibt die aktuelle Datenbank auf die Platte."""
        if not self.cache_path:
            return
        try:
            self.cache_path.parent.mkdir(parents=True, exist_ok=True)
            self.database.save_to_file(self.cache_path)
        except Exception as exc:  # pragma: no cover - Dateifehler
            LOGGER.warning("Konnte Asset-Cache nicht speichern (%s): %s", self.cache_path, exc)

    def _apply_classification(self, asset: AssetData, classification: Classification) -> None:
        """Aktualisiert ein Asset anhand einer Klassifikation."""
        asset.semantic_tags = classification.tags
        asset.semantic_profile = classification.to_profile()

    def _find_uproject(self, start_path: Path) -> Optional[Path]:
        """Sucht im angegebenen Pfad und darüber nach einer .uproject Datei."""
        current = start_path
        for _ in range(4):
            matches = list(current.glob("*.uproject"))
            if matches:
                return matches[0]
            if current.parent == current:
                break
            current = current.parent
        LOGGER.warning("Keine .uproject Datei im Pfad %s gefunden.", start_path)
        return None

    def _trigger_unreal_import(self, json_path: Path) -> None:
        if not self._ue_editor or not self._uproject_path:
            LOGGER.warning("UE-Editor oder .uproject Pfad nicht gesetzt – Import wird übersprungen.")
            return
        try:
            run_unreal_import(
                editor_exe=self._ue_editor,
                uproject=self._uproject_path,
                graph_path=json_path,
                script_path=self._ue_script,
                asset_folder=self._ue_asset_folder,
                asset_name=json_path.stem,
                map_path=self._ue_map,
                spawn=self._ue_spawn,
            )
        except Exception as exc:
            LOGGER.warning("Unreal-Import fehlgeschlagen: %s", exc)

### src\auto_pcg\terrain\__init__.py
"""Terrain-bezogene Pipelines für Heightmap-Analyse und Layer-Painting."""

from .heightmap_processor import HeightmapProcessor
from .material_planner import MaterialPlanner, TextureLibrary
from .layer_painter import LayerPainter

__all__ = ["HeightmapProcessor", "MaterialPlanner", "TextureLibrary", "LayerPainter"]

### src\auto_pcg\terrain\heightmap_processor.py
"""Heightmap-Auswertung inklusive Biome- und Settings-Empfehlungen."""

from __future__ import annotations

import io
import logging
import math
import os
import struct
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Sequence, Tuple

from auto_pcg.models.terrain import (
    BiomeLayer,
    HeightmapAnalysisResult,
    HeightmapMetadata,
    LandscapeSettings,
)

LOGGER = logging.getLogger(__name__)


@dataclass(slots=True)
class _HeightStatistics:
    min_value: float
    max_value: float
    avg_value: float
    slope_index: float
    water_ratio: float


class HeightmapProcessor:
    """Analysiert Heightmaps und erzeugt Biome-Informationen."""

    DEFAULT_SECTION_SIZE = 63
    PERFORMANCE_SECTION_SIZE = {
        "mobile": 31,
        "desktop": 63,
        "console": 63,
        "high_end_pc": 127,
    }

    def __init__(self) -> None:
        self._cached_stats: dict[Path, HeightmapAnalysisResult] = {}

    def process_heightmap(
        self,
        heightmap_path: Path,
        *,
        target_style: str = "realistic",
        performance_profile: str = "desktop",
    ) -> HeightmapAnalysisResult:
        heightmap_path = Path(heightmap_path)
        if not heightmap_path.exists():
            raise FileNotFoundError(f"Heightmap nicht gefunden: {heightmap_path}")
        cached = self._cached_stats.get(heightmap_path)
        if cached:
            return cached
        metadata = self._build_metadata(heightmap_path)
        biome_layers = self._derive_biome_layers(metadata, target_style)
        settings = self._calculate_landscape_settings(metadata, performance_profile)
        scale = self._calculate_scale(metadata)
        notes = [
            f"Heightmap {metadata.width}x{metadata.height} @ Δh {metadata.min_elevation:.1f}-{metadata.max_elevation:.1f}",
            f"Durchschnittliche Steigung: {metadata.average_slope:.2f}",
            f"Wasseranteil (niedrige Höhen): {metadata.water_ratio:.2%}",
        ]
        result = HeightmapAnalysisResult(
            metadata=metadata,
            biomes=biome_layers,
            landscape_settings=settings,
            scale=scale,
            notes=notes,
        )
        self._cached_stats[heightmap_path] = result
        return result

    # ---------------------------------------------------------------------------------- intern

    def _build_metadata(self, path: Path) -> HeightmapMetadata:
        width, height = self._extract_dimensions(path)
        stats = self._sample_height_values(path, width, height)
        return HeightmapMetadata(
            path=path,
            width=width,
            height=height,
            min_elevation=stats.min_value,
            max_elevation=stats.max_value,
            average_slope=stats.slope_index,
            water_ratio=stats.water_ratio,
        )

    def _extract_dimensions(self, path: Path) -> Tuple[int, int]:
        if path.suffix.lower() == ".png":
            try:
                with path.open("rb") as stream:
                    signature = stream.read(8)
                    if signature != b"\x89PNG\r\n\x1a\n":
                        raise ValueError("keine PNG-Signatur")
                    ihdr = stream.read(25)
                    if len(ihdr) < 25:
                        raise ValueError("ungültige IHDR-Länge")
                    width, height = struct.unpack(">II", ihdr[8:16])
                    return max(8, width), max(8, height)
            except Exception as exc:  # pragma: no cover - Headerfehler selten
                LOGGER.warning("Konnte PNG-Dimensionen nicht lesen: %s", exc)
        # Fallback: Quadratwurzel aus Dateigröße (16-bit Heightmap angenommen)
        file_size = path.stat().st_size
        pixels = max(1, file_size // 2)
        edge = int(math.sqrt(pixels))
        return max(8, edge), max(8, edge)

    def _sample_height_values(
        self,
        path: Path,
        width: int,
        height: int,
        sample_count: int = 4096,
    ) -> _HeightStatistics:
        data = path.read_bytes()
        if len(data) < 2:
            return _HeightStatistics(0.0, 0.0, 0.0, 0.0, 0.0)
        step = max(2, len(data) // sample_count)
        samples: List[int] = []
        for index in range(0, len(data) - 1, step):
            value = int.from_bytes(data[index : index + 2], "little")
            samples.append(value)
        if not samples:
            samples.append(0)
        min_value = min(samples) / 65535.0
        max_value = max(samples) / 65535.0
        avg_value = sum(samples) / (len(samples) * 65535.0)
        slope_index = self._estimate_slope(samples)
        water_threshold = min_value + (max_value - min_value) * 0.1
        water_ratio = sum(1 for value in samples if value / 65535.0 <= water_threshold) / len(samples)
        return _HeightStatistics(min_value, max_value, avg_value, slope_index, water_ratio)

    @staticmethod
    def _estimate_slope(samples: Sequence[int]) -> float:
        if len(samples) < 2:
            return 0.0
        diffs = [
            abs(samples[index] - samples[index - 1]) / 65535.0
            for index in range(1, len(samples))
        ]
        return sum(diffs) / len(diffs)

    def _derive_biome_layers(self, metadata: HeightmapMetadata, target_style: str) -> List[BiomeLayer]:
        low = (metadata.min_elevation, metadata.min_elevation + (metadata.max_elevation - metadata.min_elevation) * 0.25)
        mid = (low[1], metadata.min_elevation + (metadata.max_elevation - metadata.min_elevation) * 0.6)
        high = (mid[1], metadata.max_elevation)
        biomes = [
            BiomeLayer(
                name="water" if metadata.water_ratio > 0.15 else "valley",
                elevation_range=low,
                slope_range=(0.0, 0.3),
                water_coverage=metadata.water_ratio,
                weight=0.8,
                transitions={"forest": 0.6, "swamp": 0.2},
            ),
            BiomeLayer(
                name="forest" if target_style != "desert" else "steppe",
                elevation_range=mid,
                slope_range=(0.1, 0.6),
                water_coverage=max(0.05, metadata.water_ratio * 0.5),
                weight=1.0,
                transitions={"mountain": 0.4},
            ),
            BiomeLayer(
                name="mountain" if metadata.average_slope > 0.25 else "plateau",
                elevation_range=high,
                slope_range=(0.3, 1.0),
                water_coverage=0.05,
                weight=0.6,
                transitions={"snow": 0.3 if target_style == "tundra" else 0.1},
            ),
        ]
        return biomes

    def _calculate_landscape_settings(
        self,
        metadata: HeightmapMetadata,
        performance_profile: str,
    ) -> LandscapeSettings:
        section_size = self.PERFORMANCE_SECTION_SIZE.get(performance_profile, self.DEFAULT_SECTION_SIZE)
        components_x = max(1, metadata.width // section_size)
        components_y = max(1, metadata.height // section_size)
        lod_distance = 2000.0
        if performance_profile == "mobile":
            lod_distance = 1000.0
        elif performance_profile == "high_end_pc":
            lod_distance = 4000.0
        return LandscapeSettings(
            section_size=section_size,
            components_x=components_x,
            components_y=components_y,
            lod_distance=lod_distance,
            performance_profile=performance_profile,
        )

    def _calculate_scale(self, metadata: HeightmapMetadata) -> Tuple[float, float, float]:
        z_range = max(0.01, metadata.max_elevation - metadata.min_elevation)
        z_scale = min(256.0, max(32.0, z_range * 512.0))
        xy_scale = 100.0 if metadata.width > 2048 else 50.0
        return (xy_scale, xy_scale, z_scale)

### src\auto_pcg\terrain\layer_painter.py
"""Automatische Planung für Landscape-Layer-Masks und Adaptive Regeln."""

from __future__ import annotations

from pathlib import Path
from typing import Dict, List

from auto_pcg.models.terrain import (
    HeightmapAnalysisResult,
    LandscapeLayerMask,
    LandscapeLayerPlan,
)


class LayerPainter:
    """Leitet Layer-Masken und Adaptive-Paint-Regeln aus Biomen ab."""

    def build_layer_plan(
        self,
        analysis: HeightmapAnalysisResult,
        *,
        output_dir: Path | None = None,
    ) -> LandscapeLayerPlan:
        masks: List[LandscapeLayerMask] = []
        for order, biome in enumerate(analysis.biomes):
            mask_path = None
            if output_dir:
                mask_path = (output_dir / f"{analysis.metadata.path.stem}_{biome.name}_mask.png").resolve()
            softness = biome.transitions.get("softness", 0.25)
            masks.append(
                LandscapeLayerMask(
                    biome=biome.name,
                    mask_path=mask_path,
                    softness=softness,
                    recommended_order=order,
                )
            )
        adaptive_rules: Dict[str, float] = {
            "auto_reduce_resolution_fps": 50.0,
            "detail_fade_distance": analysis.landscape_settings.lod_distance * 0.75,
            "streaming_pool_ratio": 0.8,
        }
        if analysis.metadata.average_slope > 0.35:
            adaptive_rules["slope_stabilizer"] = analysis.metadata.average_slope
        return LandscapeLayerPlan(masks=masks, adaptive_rules=adaptive_rules)

### src\auto_pcg\terrain\material_planner.py
"""Materialplanung basierend auf Biomen und Texture-Library."""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional

from auto_pcg.models.terrain import (
    HeightmapAnalysisResult,
    MaterialBlueprint,
    MaterialLayerConfig,
)

LOGGER = logging.getLogger(__name__)


@dataclass(slots=True)
class TextureLibraryEntry:
    biome: str
    textures: Dict[str, str]


class TextureLibrary:
    """Lädt Texturdefinitionen aus JSON (oder nutzt Defaults)."""

    def __init__(self, source: Optional[Path] = None) -> None:
        self.source = source or Path(__file__).resolve().parents[1] / "data" / "texture_library.json"
        self.entries: Dict[str, TextureLibraryEntry] = {}
        self._load()

    def _load(self) -> None:
        if not self.source.exists():
            LOGGER.warning("Texture-Library nicht gefunden: %s", self.source)
            return
        payload = json.loads(self.source.read_text(encoding="utf-8"))
        for biome, textures in payload.items():
            if not isinstance(textures, dict):
                continue
            self.entries[biome.lower()] = TextureLibraryEntry(biome=biome, textures=textures)

    def find_best_match(self, biome: str, fallback: Optional[str] = None) -> Dict[str, str]:
        biome = biome.lower()
        if biome in self.entries:
            return self.entries[biome].textures
        if fallback and fallback.lower() in self.entries:
            return self.entries[fallback.lower()].textures
        if self.entries:
            return next(iter(self.entries.values())).textures
        return {}


class MaterialPlanner:
    """Baut Master-Material-Konfigurationen für Landscape-Biome."""

    def __init__(self, texture_library: Optional[TextureLibrary] = None) -> None:
        self.library = texture_library or TextureLibrary()

    def build_blueprint(
        self,
        analysis: HeightmapAnalysisResult,
        *,
        target_style: str = "realistic",
        performance_profile: str = "desktop",
    ) -> MaterialBlueprint:
        layers: List[MaterialLayerConfig] = []
        for biome in analysis.biomes:
            texture_set = self.library.find_best_match(biome.name, fallback="forest")
            tiling = self._derive_tiling(texture_set, analysis, biome)
            blending_rules = self._build_blending_rules(biome, target_style)
            parameters = self._expose_parameters(biome, performance_profile)
            layers.append(
                MaterialLayerConfig(
                    biome=biome.name,
                    texture_set=texture_set,
                    tiling=tiling,
                    blending_rules=blending_rules,
                    exposed_parameters=parameters,
                )
            )
        global_parameters = {
            "macro_variation_intensity": 0.35 if target_style == "stylized" else 0.2,
            "triplanar_blend": 0.5 if analysis.metadata.average_slope > 0.3 else 0.3,
            "runtime_virtual_texture": 1.0 if performance_profile in {"desktop", "high_end_pc"} else 0.0,
        }
        performance_notes = [
            f"{len(layers)} Layer, Section-Size {analysis.landscape_settings.section_size}",
            f"Empfohlene LOD-Distanz: {analysis.landscape_settings.lod_distance}",
        ]
        return MaterialBlueprint(
            layers=layers,
            global_parameters=global_parameters,
            performance_notes=performance_notes,
        )

    def _derive_tiling(
        self,
        texture_set: Dict[str, str],
        analysis: HeightmapAnalysisResult,
        biome,
    ) -> float:
        ratio = analysis.metadata.width / max(analysis.metadata.height, 1)
        base = 2.0 if ratio > 1.2 else 1.0
        if "rock" in biome.name.lower():
            base *= 0.5
        if texture_set.get("albedo", "").lower().endswith("sand_albedo"):
            base *= 4.0
        return round(base, 2)

    def _build_blending_rules(self, biome, target_style: str) -> Dict[str, float]:
        softness = 0.35 if target_style == "stylized" else 0.2
        contrast = 1.2 if target_style == "stylized" else 0.8
        return {
            "slope_bias": sum(biome.slope_range) / 2,
            "transition_softness": softness,
            "detail_contrast": contrast,
        }

    def _expose_parameters(self, biome, performance_profile: str) -> Dict[str, float]:
        detail_intensity = 0.7 if performance_profile == "high_end_pc" else 0.4
        if biome.name.lower() in {"desert", "steppe"}:
            detail_intensity *= 0.5
        return {
            "detail_normal_strength": detail_intensity,
            "roughness_min": 0.2,
            "roughness_max": 0.8,
        }

